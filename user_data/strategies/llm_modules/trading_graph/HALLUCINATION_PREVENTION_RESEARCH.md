# ğŸ”¬ LangGraph Agents é˜²å¹»è§‰è®¾è®¡ç ”ç©¶æŠ¥å‘Š

> åˆ›å»ºæ—¶é—´: 2025-12-06
> åŸºäº: Tavily æ·±åº¦æœç´¢ç ”ç©¶
> ç›®çš„: å¯¹æ¯”è¡Œä¸šæœ€ä½³å®è·µï¼Œæ”¹è¿›æœ¬é¡¹ç›®çš„é˜²å¹»è§‰æ¶æ„

---

## ğŸ“‹ ç›®å½•

1. [ç ”ç©¶æ¦‚è¿°](#ç ”ç©¶æ¦‚è¿°)
2. [å…³é”®å‘ç°ï¼šLLM è‡ªæˆ‘éªŒè¯æŠ€æœ¯](#å…³é”®å‘ç°llm-è‡ªæˆ‘éªŒè¯æŠ€æœ¯)
3. [è¡Œä¸šæœ€ä½³å®è·µæ±‡æ€»](#è¡Œä¸šæœ€ä½³å®è·µæ±‡æ€»)
4. [æœ¬é¡¹ç›®ç°çŠ¶åˆ†æ](#æœ¬é¡¹ç›®ç°çŠ¶åˆ†æ)
5. [æ”¹è¿›å»ºè®®ä¸å®æ–½æ–¹æ¡ˆ](#æ”¹è¿›å»ºè®®ä¸å®æ–½æ–¹æ¡ˆ)
6. [ä¼˜å…ˆçº§æ’åº](#ä¼˜å…ˆçº§æ’åº)
7. [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## ç ”ç©¶æ¦‚è¿°

### ç ”ç©¶æ–¹æ³•

é€šè¿‡ Tavily è¿›è¡Œäº† 12+ æ¬¡æ·±åº¦æœç´¢ï¼Œè¦†ç›–ä»¥ä¸‹ä¸»é¢˜ï¼š

- LLM agent hallucination prevention best practices
- Multi-agent debate adversarial verification
- Verification-First (VF) prompting strategy
- Self-consistency and self-refine techniques
- Neuro-symbolic AI hybrid systems
- LLM output structured validation
- Financial/trading LLM decision making risks

### æ ¸å¿ƒå‘ç°

1. **Verification-First (VF)** æ˜¯ä¸€ç§å‡ ä¹"å…è´¹åˆé¤"çš„æå‡æ–¹æ³•
2. **å¤š Agent å¯¹æŠ—æ€§è¾©è®º** å¯å‡å°‘ 30-50% è¿‡åº¦è‡ªä¿¡
3. **ç¥ç»ç¬¦å·æ··åˆ** æ˜¯ä¼ä¸šçº§å¯é æ€§çš„å…³é”®
4. **LLM åœ¨æ•°å€¼æ¨ç†ä¸Šå®¹æ˜“å¹»è§‰**ï¼Œé‡‘èé¢†åŸŸåº”è®©ä»£ç è®¡ç®—æ•°å€¼

---

## å…³é”®å‘ç°ï¼šLLM è‡ªæˆ‘éªŒè¯æŠ€æœ¯

### 1. Verification-First (VF) éªŒè¯ä¼˜å…ˆç­–ç•¥ â­â­â­

**è®ºæ–‡**: "Asking LLMs to Verify First is Almost Free Lunch" (arXiv 2511.21734)

**æ ¸å¿ƒåŸç†**:

- è®© LLM å…ˆéªŒè¯ä¸€ä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
- è§¦å‘ "**é€†å‘æ¨ç†**" (Reverse Reasoning) è¿‡ç¨‹
- éªŒè¯æ¯”ç”Ÿæˆåœ¨è®¤çŸ¥ä¸Šæ›´ç®€å•ï¼Œä¸æ­£å‘ CoT äº’è¡¥

**å…³é”®å‘ç°**:
> "Verifying an answer is easier than generating the correct answer."
> "éªŒè¯ç­”æ¡ˆæ¯”ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆæ›´å®¹æ˜“ã€‚"

**å·¥ä½œåŸç†**:

```
ä¼ ç»Ÿ CoT: "é€æ­¥æ€è€ƒæ‰¾å‡ºç­”æ¡ˆ"
VF ç­–ç•¥:  "å…ˆéªŒè¯è¿™ä¸ªç­”æ¡ˆæ˜¯å¦æ­£ç¡®: [å€™é€‰ç­”æ¡ˆ]ï¼Œç„¶åç»™å‡ºæ­£ç¡®ç­”æ¡ˆ"
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ** (åŸºäºè®¤çŸ¥ç§‘å­¦):

1. **é€†å‘æ¨ç†**: ä»æ½œåœ¨ç»“è®ºå›æº¯åˆ°å‰æï¼Œåˆ©ç”¨ Polya é—®é¢˜è§£å†³æ³•çš„ "æ£€éªŒ" é˜¶æ®µ
2. **å‡å°‘æœç´¢ç©ºé—´**: å³ä½¿å€™é€‰ç­”æ¡ˆé”™è¯¯ï¼Œé€†å‘è·¯å¾„ä¹Ÿæä¾›äº†è„šæ‰‹æ¶
3. **æ¿€æ´»æ‰¹åˆ¤æ€§æ€ç»´**: LLM ä½œä¸ºæ‰¹è¯„è€…å®¡è§†é—®é¢˜

**ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”**:

| æ–¹æ³•                   | ç­–ç•¥         | ä¿¡æ¯è€ƒè™‘       | Token æˆæœ¬ |
| ---------------------- | ------------ | -------------- | ---------- |
| Self-Correction        | åæ€ + æ”¹è¿›  | æ‰€æœ‰å†å²ä¸Šä¸‹æ–‡ | é«˜         |
| Self-Consistency       | å¤šæ¬¡é‡‡æ ·æŠ•ç¥¨ | å¤šæ¡æ¨ç†é“¾     | å¾ˆé«˜       |
| **Verification-First** | å…ˆéªŒè¯å†ç”Ÿæˆ | ä»…å€™é€‰ç­”æ¡ˆ     | **ä½**     |
| Iter-VF                | è¿­ä»£éªŒè¯     | ä»…ä¸Šä¸€æ­¥ç­”æ¡ˆ   | ä¸­ç­‰       |

**é€‚ç”¨æœ¬é¡¹ç›®**:

```python
# Executor Agent åº”ç”¨ VF ç­–ç•¥
EXECUTOR_VF_PROMPT = """
åœ¨åšå‡ºæœ€ç»ˆäº¤æ˜“å†³ç­–å‰ï¼Œè¯·å…ˆéªŒè¯ä»¥ä¸‹ç”±å‰åº agents å¾—å‡ºçš„åˆæ­¥ç»“è®º:

åˆæ­¥ç»“è®º:
- æ–¹å‘: {consensus_direction}
- ç½®ä¿¡åº¦: {consensus_confidence}%
- Judge è£å†³: {judge_verdict}

éªŒè¯ä»»åŠ¡:
1. è¿™ä¸ªæ–¹å‘åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®æ”¯æ’‘ï¼Ÿ
2. ç½®ä¿¡åº¦æ˜¯å¦ä¸è¯æ®å¼ºåº¦åŒ¹é…ï¼Ÿ
3. æ˜¯å¦å­˜åœ¨è¢«å¿½ç•¥çš„é‡å¤§é£é™©ï¼Ÿ

éªŒè¯å®Œæˆåï¼Œç»™å‡ºä½ çš„æœ€ç»ˆå†³ç­–ã€‚
"""
```

---

### 2. Self-Consistency è‡ªä¸€è‡´æ€§ â­â­

**æ¥æº**: Wang et al. (2022), Microsoft Research

**æ ¸å¿ƒåŸç†**:

- ç”¨è¾ƒé«˜ temperature ç”Ÿæˆå¤šä¸ªå“åº” (5-20 ä¸ª)
- é€šè¿‡æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆ

**æ•ˆæœ**:

- åœ¨æ¨ç†ä»»åŠ¡ä¸Šæå‡ 15-25% å‡†ç¡®ç‡
- å¤æ‚é—®é¢˜æå‡æ›´æ˜¾è‘—

**ç¼ºç‚¹**:

- è®¡ç®—æˆæœ¬é«˜ (5-20x API è°ƒç”¨)
- ä¸€é¡¹æµ‹è¯•æ˜¾ç¤º "æœ€å·® ROI æŠ€æœ¯" (åœ¨æŸäº›åœºæ™¯)

**é€‚ç”¨åœºæ™¯**:

- é«˜é£é™©å†³ç­– (é”™è¯¯æˆæœ¬ >> API æˆæœ¬)
- é‡‘èç›‘ç®¡åˆè§„åˆ†æ

**é€‚ç”¨æœ¬é¡¹ç›®**:

```python
def executor_with_self_consistency(state, n_samples=3, temperature=0.7):
    """é«˜é£é™©å†³ç­–ä½¿ç”¨è‡ªä¸€è‡´æ€§éªŒè¯"""
    decisions = []
    for _ in range(n_samples):
        decision = executor_agent_node(state, temperature=temperature)
        decisions.append(decision)
    
    # æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„è¡ŒåŠ¨
    actions = [d.get("final_action") for d in decisions]
    most_common = Counter(actions).most_common(1)[0]
    
    if most_common[1] < n_samples * 0.6:  # ä¸€è‡´æ€§ä½äº 60%
        logger.warning(f"Low consistency: {actions}")
        return {"final_action": "signal_wait", "reason": "Inconsistent decisions"}
    
    return decisions[actions.index(most_common[0])]
```

---

### 3. Self-Refine / Reflexion è‡ªæ”¹è¿› â­â­

**æ¥æº**: Madaan et al. (2023), Shinn et al. (2023)

**æ ¸å¿ƒåŸç†**:

```
[ç”Ÿæˆ] â†’ [æ‰¹è¯„] â†’ [æ”¹è¿›] â†’ é‡å¤ç›´åˆ°æ»¡æ„
```

**Spring AI å®ç°æ¨¡å¼**:

```
ç”Ÿæˆå“åº” â†’ è¯„ä¼°è´¨é‡ â†’ å¦‚æœå¤±è´¥åˆ™å¸¦åé¦ˆé‡è¯• â†’ è¾¾åˆ°è´¨é‡é˜ˆå€¼æˆ–é‡è¯•é™åˆ¶
```

**è¯„ä¼°ç»´åº¦** (1-5 åˆ†åˆ¶):

- 5 = å®Œç¾éµå¾ªæ‰€æœ‰æŒ‡ä»¤
- 4 = å¤§éƒ¨åˆ†éµå¾ªï¼Œè½»å¾®åå·®
- 3 = éƒ¨åˆ†éµå¾ªï¼Œéƒ¨åˆ†å¿½ç•¥
- 2 = å°‘é‡éµå¾ª
- 1 = åŸºæœ¬å¿½ç•¥

**é€‚ç”¨æœ¬é¡¹ç›®**:

```python
def executor_with_self_refine(state, max_retries=2):
    """å¸¦è‡ªæ”¹è¿›çš„ Executor"""
    decision = executor_agent_node(state)
    
    for attempt in range(max_retries):
        # è¯„ä¼°å†³ç­–è´¨é‡
        quality_score = evaluate_decision_quality(decision, state)
        
        if quality_score >= 4:
            return decision
        
        # ç”Ÿæˆæ”¹è¿›åé¦ˆ
        feedback = generate_improvement_feedback(decision, state, quality_score)
        
        # å¸¦åé¦ˆé‡æ–°ç”Ÿæˆ
        decision = executor_agent_with_feedback(state, feedback)
    
    return decision

def evaluate_decision_quality(decision, state):
    """LLM-as-Judge è¯„ä¼°å†³ç­–è´¨é‡"""
    prompt = f"""
    è¯„ä¼°ä»¥ä¸‹äº¤æ˜“å†³ç­–çš„è´¨é‡ (1-5åˆ†):
    
    å†³ç­–: {decision}
    å¸‚åœºçŠ¶æ€: {state.get("market_context")}
    
    è¯„ä¼°ç»´åº¦:
    1. æ˜¯å¦è€ƒè™‘äº† Grounding çº æ­£åçš„æ•°æ®ï¼Ÿ
    2. æ­¢æŸ/æ­¢ç›ˆæ˜¯å¦è®¾ç½®åˆç†ï¼Ÿ
    3. ç½®ä¿¡åº¦æ˜¯å¦ä¸è¯æ®åŒ¹é…ï¼Ÿ
    4. é£é™©è¯„ä¼°æ˜¯å¦å®Œæ•´ï¼Ÿ
    
    ä»…è¾“å‡ºåˆ†æ•° (1-5):
    """
    return int(llm.invoke(prompt))
```

---

### 4. LLM-as-Judge æ¨¡å¼ â­â­â­

**æ¥æº**: Gu, Jiawei et al. (2024) "A Survey On LLM-as-a-Judge"

**æ ¸å¿ƒæ¨¡å¼**:

- **ç›´æ¥è¯„ä¼°** (Direct Assessment): é€ç‚¹è¯„åˆ† (1-5)
- **æˆå¯¹æ¯”è¾ƒ** (Pairwise Comparison): ä¸¤ä¸ªè¾“å‡ºæ¯”è¾ƒ
- **å‚è€ƒåŸºå‡†** (Reference-based): ä¸é»„é‡‘æ ‡å‡†æ¯”è¾ƒ

**æœ€ä½³å®è·µ**:

```
## è¯„ä¼°æŒ‡ä»¤:
è¯„ä¼°è¾“å‡ºå¯¹è¾“å…¥çš„å›åº”ç¨‹åº¦ï¼Œåˆ†æå“åº”å†…å®¹çš„ç›¸å…³æ€§ã€‚

è¯„åˆ†æ ‡å‡† (1-5):
- 5 = è¾“å‡ºå®Œç¾å›åº”è¾“å…¥ï¼Œæ‰€æœ‰å†…å®¹ç›¸å…³
- 4 = è¾“å‡ºå¤§éƒ¨åˆ†å›åº”è¾“å…¥ï¼Œæœ‰è½»å¾®æ— å…³ç»†èŠ‚
- 3 = è¾“å‡ºéƒ¨åˆ†å›åº”è¾“å…¥ï¼Œæœ‰ä¸€äº›æ— å…³å†…å®¹
- 2 = è¾“å‡ºå‹‰å¼ºå›åº”è¾“å…¥ï¼Œå¤§éƒ¨åˆ†æ— å…³
- 1 = è¾“å‡ºåŸºæœ¬æ²¡æœ‰å›åº”è¾“å…¥
```

**é€‚ç”¨æœ¬é¡¹ç›®**:
åœ¨ Executor Agent åå¢åŠ  Judge è¯„ä¼°å±‚:

```python
def post_executor_judge(decision, state):
    """Executor å†³ç­–åç½®è¯„å®¡"""
    eval_prompt = f"""
    ä½œä¸ºç‹¬ç«‹è¯„å®¡å‘˜ï¼Œè¯„ä¼°ä»¥ä¸‹äº¤æ˜“å†³ç­–:
    
    å†³ç­–: {decision}
    
    è¯„ä¼°ç»´åº¦:
    1. ä¸€è‡´æ€§ (1-5): å†³ç­–æ˜¯å¦ä¸å‰åº agents åˆ†æä¸€è‡´ï¼Ÿ
    2. é£æ§å®Œæ•´æ€§ (1-5): æ­¢æŸæ­¢ç›ˆæ˜¯å¦è®¾ç½®ï¼Ÿé£é™©è¯„ä¼°æ˜¯å¦å®Œæ•´ï¼Ÿ
    3. æ•°æ®ä¾æ® (1-5): å†³ç­–æ˜¯å¦åŸºäºçº æ­£åçš„çœŸå®æ•°æ®ï¼Ÿ
    
    å¦‚æœä»»ä½•ç»´åº¦ < 3ï¼Œå»ºè®®æ‹’ç»æ­¤å†³ç­–ã€‚
    
    è¾“å‡ºæ ¼å¼:
    ä¸€è‡´æ€§: X/5
    é£æ§å®Œæ•´æ€§: X/5
    æ•°æ®ä¾æ®: X/5
    å»ºè®®: APPROVE / REJECT
    """
    return llm.invoke(eval_prompt)
```

---

## è¡Œä¸šæœ€ä½³å®è·µæ±‡æ€»

### 1. å¤šå±‚é˜²å¹»è§‰æ¶æ„

**Amazon Neuro-Symbolic Approach**:

```
Policy â†’ å½¢å¼é€»è¾‘ç¿»è¯‘ â†’ LLM å“åº” â†’ å½¢å¼é€»è¾‘ç¿»è¯‘ â†’ è‡ªåŠ¨æ¨ç†éªŒè¯ â†’ é€šè¿‡/é‡è¯•
```

**EY Knowledge Graph**:

```
æ·±åº¦å­¦ä¹  + çŸ¥è¯†å›¾è°± â†’ è¯­ä¹‰ä¸€è‡´æ€§éªŒè¯
```

**ç ”ç©¶ç»“è®º**:
> "Neuro-symbolic AI combines deep learning's pattern recognition with logic-based validation."

### 2. RAG æ¶æ„æ ¸å¿ƒè¦ç´ 

**Glean / Oracle / Elasticsearch å®è·µ**:

- **å‘é‡æ•°æ®åº“**: å¿«é€Ÿç²¾ç¡®æ£€ç´¢
- **Reference Linking**: å“åº”é“¾æ¥åˆ°åŸå§‹æ–‡æ¡£
- **å®æ—¶è®¿é—®**: é¿å…ä¾èµ–è®­ç»ƒæ•°æ®

**å…³é”®åŸåˆ™**:
> "Garbage in, garbage out â€” RAG å‡å°‘ä½†ä¸æ¶ˆé™¤å¹»è§‰"

### 3. ç»“æ„åŒ–è¾“å‡ºä¿è¯

**Anthropic / AWS Bedrock**:

- API å±‚é¢ JSON Schema å¼ºåˆ¶
- æ¶ˆé™¤è§£æé”™è¯¯å’Œé‡è¯•é€»è¾‘

**Pydantic éªŒè¯**:

```python
from pydantic import BaseModel, Field, validator

class TradingDecision(BaseModel):
    action: str = Field(..., pattern=r"^(signal_entry_long|signal_wait|...)$")
    confidence: float = Field(..., ge=0, le=100)
    
    @validator('stop_loss_price')
    def validate_stop_loss(cls, v, values):
        if values.get('action').startswith('signal_entry'):
            if v is None:
                raise ValueError('Entry requires stop loss')
        return v
```

### 4. é‡‘èé¢†åŸŸç‰¹æ®Šè€ƒè™‘

**arXiv 2512.01123 ç ”ç©¶**:
> "LLMs often produce plausible but mathematically incorrect calculations,
> especially with compound probabilities, expected values, and risk assessments."

**æ¨èæ–¹æ¡ˆ**:

- ç”¨ LLM ä½œä¸º "æ™ºèƒ½æ¨¡å‹æ„å»ºå™¨"
- è®©ç»“æ„åŒ–æ¨¡å‹ (å¦‚è´å¶æ–¯ç½‘ç»œ) åšæœ€ç»ˆå†³ç­–
- ç»“æœ: Sharpe ratio 1.08, Max drawdown -8.2%

### 5. å¯è§‚æµ‹æ€§è¦æ±‚

**Dynatrace / McKinsey 2025**:

- Token æ¶ˆè€—è¿½è¸ª
- æ¨¡å‹è¡Œä¸ºç›‘æ§
- Guardrail ç»“æœè®°å½•
- éçº¿æ€§æµè¿½è¸ª

> "McKinsey: æ²»ç†å’Œé£é™©ç®¡ç†å·¥å…·ç¼ºå¤±æ˜¯ AI é‡‡ç”¨çš„ #1 éšœç¢"

### 6. ç½®ä¿¡åº¦æ ¡å‡†

**SeSE æ¡†æ¶** (arXiv 2511.16275):

- é€šè¿‡å¤šæ¬¡é‡‡æ ·çš„ç»“æ„ç†µé‡åŒ–è¯­ä¹‰ä¸ç¡®å®šæ€§
- é›¶èµ„æºï¼Œä»…éœ€é‡‡æ ·å“åº”

**åŒ»å­¦/é‡‘èé¢†åŸŸ**:
> "LLM ç½®ä¿¡åº¦é€šå¸¸è¿‡é«˜ï¼Œéœ€è¦å¤–éƒ¨æ ¡å‡†"

---

## æœ¬é¡¹ç›®ç°çŠ¶åˆ†æ

### âœ… å·²å®ç°çš„æœ€ä½³å®è·µ

| å®è·µ                | å®ç°ä½ç½®            | è¯„ä¼°                  |
| ------------------- | ------------------- | --------------------- |
| å¤š Agent å¯¹æŠ—æ€§è¾©è®º | Bull/Bear/Judge     | âœ… å®Œæ•´å®ç°            |
| Grounding éªŒè¯      | grounding_node.py   | âš ï¸ éƒ¨åˆ†å®ç° (æ–‡æœ¬è§£æ) |
| å¹»è§‰é˜ˆå€¼æˆªæ–­        | routing.py (70%)    | âœ… å·²å®ç°              |
| ä¿å®ˆå›é€€æœºåˆ¶        | executor_agent.py   | âœ… å·²å®ç°              |
| ç½®ä¿¡åº¦æ ¡å‡†          | judge_node.py       | âš ï¸ ç®€å•å¹³å‡            |
| å†³ç­–æ—¥å¿—            | GraphDecisionLogger | âœ… å·²å®ç°              |

### âŒ ç¼ºå¤±çš„æœ€ä½³å®è·µ

| å®è·µ                        | å½±å“                 | ä¼˜å…ˆçº§ |
| --------------------------- | -------------------- | ------ |
| **Verification-First (VF)** | å‡ ä¹å…è´¹çš„å‡†ç¡®ç‡æå‡ | ğŸ”´ é«˜   |
| **ç»“æ„åŒ–æ•°æ®æº**            | é¿å…æ–‡æœ¬è§£æé”™è¯¯     | ğŸ”´ é«˜   |
| **Pydantic éªŒè¯**           | æ¶ˆé™¤è§£æå¤±è´¥         | ğŸ”´ é«˜   |
| **LLM æ•°å€¼åˆ†ç¦»**            | é¿å…æ•°å€¼å¹»è§‰         | ğŸ”´ é«˜   |
| **è‡ªä¸€è‡´æ€§é‡‡æ ·**            | é«˜é£é™©å†³ç­–éªŒè¯       | ğŸŸ¡ ä¸­   |
| **æ¨ç†ä¸€è‡´æ€§éªŒè¯**          | æ£€æµ‹é€»è¾‘çŸ›ç›¾         | ğŸŸ¡ ä¸­   |
| **å®Œæ•´å¯è§‚æµ‹æ€§**            | ç›‘æ§å’Œè°ƒä¼˜           | ğŸŸ¡ ä¸­   |
| **Post-Executor Judge**     | å†³ç­–åç½®è¯„å®¡         | ğŸŸ¢ ä½   |

---

## æ”¹è¿›å»ºè®®ä¸å®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: Verification-First (VF) é›†æˆ ğŸ”´

**ä½ç½®**: `nodes/execution/executor_agent.py`

**å®ç°**:

```python
EXECUTOR_VF_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“æ‰§è¡Œä¸“å®¶ã€‚

åœ¨åšå‡ºæœ€ç»ˆå†³ç­–å‰ï¼Œä½ å¿…é¡»å…ˆéªŒè¯å‰åº agents çš„ç»“è®ºã€‚

<éªŒè¯ä»»åŠ¡>
1. éªŒè¯åˆ†æå…±è¯†æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®æ”¯æ’‘
2. éªŒè¯è¾©è®ºç»“è®ºæ˜¯å¦é€»è¾‘è‡ªæ´½
3. éªŒè¯ Grounding çº æ­£åçš„æ•°æ®æ˜¯å¦è¢«æ­£ç¡®å¼•ç”¨
4. è¯†åˆ«ä»»ä½•è¢«å¿½ç•¥çš„é‡å¤§é£é™©
</éªŒè¯ä»»åŠ¡>

<è¾“å‡ºæ ¼å¼>
[éªŒè¯ç»“æœ]
åˆ†æå…±è¯†éªŒè¯: PASS/FAIL (ç†ç”±)
è¾©è®ºç»“è®ºéªŒè¯: PASS/FAIL (ç†ç”±)
æ•°æ®å¼•ç”¨éªŒè¯: PASS/FAIL (ç†ç”±)
é£é™©è¯†åˆ«: æ— é—æ¼ / å‘ç°é—æ¼: [...]

[éªŒè¯åå†³ç­–]
action: ...
confidence: ...
...
</è¾“å‡ºæ ¼å¼>
"""

def executor_with_verification_first(state):
    """å¸¦ VF ç­–ç•¥çš„ Executor"""
    # æ„å»ºå€™é€‰ç­”æ¡ˆ (æ¥è‡ª Judge è£å†³)
    judge_verdict = state.get("judge_verdict") or state.get("position_judge_verdict")
    
    candidate_answer = {
        "direction": state.get("consensus_direction"),
        "confidence": state.get("consensus_confidence"),
        "verdict": judge_verdict.verdict if judge_verdict else "unknown"
    }
    
    user_prompt = f"""
    è¯·å…ˆéªŒè¯ä»¥ä¸‹å€™é€‰ç»“è®ºï¼Œç„¶åç»™å‡ºä½ çš„æœ€ç»ˆå†³ç­–:
    
    å€™é€‰ç»“è®º:
    - æ–¹å‘: {candidate_answer['direction']}
    - ç½®ä¿¡åº¦: {candidate_answer['confidence']}%
    - Judge è£å†³: {candidate_answer['verdict']}
    
    [å¼€å§‹éªŒè¯]
    """
    
    # è°ƒç”¨ LLM
    response = llm.invoke([
        {"role": "system", "content": EXECUTOR_VF_SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt + _build_context(state)}
    ])
    
    # è§£æéªŒè¯ç»“æœ + å†³ç­–
    return _parse_vf_response(response.content)
```

---

### æ–¹æ¡ˆ 2: ç»“æ„åŒ–æ•°æ®æº ğŸ”´

**ä½ç½®**: `state.py`, `grounding_node.py`

**å®ç°**:

```python
# state.py - æ–°å¢å­—æ®µ
class TradingDecisionState(TypedDict, total=False):
    # ... ç°æœ‰å­—æ®µ ...
    
    # æ–°å¢: ç»“æ„åŒ–æŒ‡æ ‡æ•°æ® (ç›´æ¥ä» ContextBuilder ä¼ é€’)
    verified_indicator_data: Optional[Dict[str, float]]
    # ç¤ºä¾‹: {"RSI": 45.2, "ADX": 32.1, "MACD": 0.0025, "MFI": 55.8}

# grounding_node.py - ä½¿ç”¨ç»“æ„åŒ–æ•°æ®
def _extract_actual_values(state: TradingDecisionState) -> Dict[str, float]:
    """ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–æ•°æ®ï¼Œå›é€€åˆ°æ–‡æœ¬è§£æ"""
    # ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–æ•°æ®
    verified_data = state.get("verified_indicator_data")
    if verified_data and len(verified_data) > 0:
        logger.debug(f"[GroundingNode] Using verified_indicator_data: {verified_data}")
        return verified_data
    
    # å›é€€åˆ°æ–‡æœ¬è§£æ (å…¼å®¹æ€§)
    logger.warning("[GroundingNode] Falling back to text parsing")
    return _extract_from_market_context(state.get("market_context", ""))

# langgraph_client.py - ä¼ é€’ç»“æ„åŒ–æ•°æ®
def build_initial_state(market_data, indicators):
    return TradingDecisionState(
        # ... å…¶ä»–å­—æ®µ ...
        verified_indicator_data={
            "RSI": indicators.get("rsi_14", 50),
            "ADX": indicators.get("adx_14", 25),
            "MACD": indicators.get("macd_hist", 0),
            "MFI": indicators.get("mfi_14", 50),
            "STOCH_K": indicators.get("stoch_k", 50),
            "STOCH_D": indicators.get("stoch_d", 50),
        }
    )
```

---

### æ–¹æ¡ˆ 3: Pydantic éªŒè¯ ğŸ”´

**ä½ç½®**: `nodes/execution/executor_agent.py`

**å®ç°**:

```python
from pydantic import BaseModel, Field, validator, root_validator
from typing import Optional, Literal

class ExecutorOutputSchema(BaseModel):
    """Executor Agent è¾“å‡ºçš„ä¸¥æ ¼ Schema"""
    
    action: Literal[
        "signal_entry_long", 
        "signal_entry_short", 
        "signal_wait", 
        "signal_hold", 
        "signal_exit", 
        "adjust_position"
    ]
    confidence: float = Field(..., ge=0, le=100)
    leverage: Optional[int] = Field(None, ge=1, le=100)
    direction: Optional[Literal["long", "short", "neutral"]] = None
    
    # é£æ§å­—æ®µ
    stop_loss_price: Optional[float] = Field(None, gt=0)
    take_profit_price: Optional[float] = Field(None, gt=0)
    risk_reward_ratio: Optional[float] = Field(None, ge=0.5, le=10)
    
    # è°ƒæ•´å­—æ®µ
    adjustment_pct: Optional[float] = Field(None, ge=-70, le=50)
    adjustment_type: Optional[Literal["scale_in", "partial_exit"]] = None
    
    # æ¨ç†
    reasoning: str = ""
    
    @root_validator
    def validate_entry_requirements(cls, values):
        action = values.get("action")
        if action in ["signal_entry_long", "signal_entry_short"]:
            if not values.get("stop_loss_price"):
                raise ValueError(f"{action} requires stop_loss_price")
            if not values.get("take_profit_price"):
                raise ValueError(f"{action} requires take_profit_price")
            if values.get("confidence", 0) < 60:
                raise ValueError(f"{action} requires confidence >= 60")
        return values
    
    @root_validator
    def validate_adjustment_requirements(cls, values):
        if values.get("action") == "adjust_position":
            if not values.get("adjustment_pct"):
                raise ValueError("adjust_position requires adjustment_pct")
            if not values.get("adjustment_type"):
                raise ValueError("adjust_position requires adjustment_type")
        return values

def _parse_executor_response_v2(response_text: str, state: dict) -> ExecutorOutputSchema:
    """ä½¿ç”¨ Pydantic è§£æå’ŒéªŒè¯ Executor è¾“å‡º"""
    try:
        # å°è¯• JSON è§£æ
        import json
        import re
        
        # æå– JSON å—
        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            return ExecutorOutputSchema(**data)
    except (json.JSONDecodeError, ValidationError) as e:
        logger.warning(f"[ExecutorAgent] Pydantic validation failed: {e}")
    
    # å›é€€åˆ°æ­£åˆ™è§£æ
    parsed = _parse_executor_response_legacy(response_text)
    
    try:
        return ExecutorOutputSchema(**parsed)
    except ValidationError as e:
        logger.error(f"[ExecutorAgent] Legacy parse also failed validation: {e}")
        # è¿”å›ä¿å®ˆé»˜è®¤å€¼
        return ExecutorOutputSchema(
            action="signal_wait" if not state.get("has_position") else "signal_hold",
            confidence=0,
            reasoning=f"Validation failed: {e}"
        )
```

---

### æ–¹æ¡ˆ 4: LLM æ•°å€¼åˆ†ç¦» ğŸ”´

**åŸç†**: LLM åªè¾“å‡ºå®šæ€§åˆ¤æ–­ï¼Œä»£ç è®¡ç®—ç²¾ç¡®æ•°å€¼

**ä½ç½®**: `nodes/execution/executor_agent.py`

**å®ç°**:

```python
class ExecutorQualitativeOutput(BaseModel):
    """LLM åªè¾“å‡ºå®šæ€§åˆ¤æ–­"""
    action: str
    direction_strength: Literal["strong", "moderate", "weak"]
    risk_level: Literal["high", "medium", "low"]
    confidence: float
    reasoning: str
    key_factors: list[str] = []
    
    # ä¸è®© LLM è¾“å‡ºæ•°å€¼ï¼
    # stop_loss_price: float  âŒ ç§»é™¤
    # take_profit_price: float  âŒ ç§»é™¤

def calculate_risk_management(
    qualitative: ExecutorQualitativeOutput,
    current_price: float,
    key_support: Optional[float],
    key_resistance: Optional[float],
    risk_config: dict
) -> dict:
    """ä»£ç å±‚è®¡ç®—ç²¾ç¡®çš„é£æ§å‚æ•°"""
    
    # åŸºäºé£é™©ç­‰çº§ç¡®å®šæ­¢æŸç™¾åˆ†æ¯”
    sl_pct_map = {
        "high": risk_config.get("high_risk_sl_pct", 0.015),   # 1.5%
        "medium": risk_config.get("medium_risk_sl_pct", 0.025), # 2.5%
        "low": risk_config.get("low_risk_sl_pct", 0.04),      # 4%
    }
    sl_pct = sl_pct_map[qualitative.risk_level]
    
    # åŸºäºæ–¹å‘å¼ºåº¦ç¡®å®šæ­¢ç›ˆå€æ•°
    tp_multiplier_map = {
        "strong": 3.0,   # 3:1 RR
        "moderate": 2.0, # 2:1 RR
        "weak": 1.5,     # 1.5:1 RR
    }
    tp_multiplier = tp_multiplier_map[qualitative.direction_strength]
    
    if qualitative.action == "signal_entry_long":
        # æ­¢æŸå– (å½“å‰ä»· - sl_pct%) å’Œ (æ”¯æ’‘ä½ - 0.5%) çš„è¾ƒé«˜è€…
        stop_loss = current_price * (1 - sl_pct)
        if key_support:
            stop_loss = max(stop_loss, key_support * 0.995)
        
        # æ­¢ç›ˆåŸºäºé£é™©å›æŠ¥æ¯”
        risk = current_price - stop_loss
        take_profit = current_price + (risk * tp_multiplier)
        if key_resistance:
            take_profit = min(take_profit, key_resistance * 1.005)
    
    elif qualitative.action == "signal_entry_short":
        stop_loss = current_price * (1 + sl_pct)
        if key_resistance:
            stop_loss = min(stop_loss, key_resistance * 1.005)
        
        risk = stop_loss - current_price
        take_profit = current_price - (risk * tp_multiplier)
        if key_support:
            take_profit = max(take_profit, key_support * 0.995)
    else:
        return {}
    
    actual_rr = abs(take_profit - current_price) / abs(current_price - stop_loss)
    
    return {
        "stop_loss_price": round(stop_loss, 2),
        "take_profit_price": round(take_profit, 2),
        "stop_loss_pct": round(sl_pct * 100, 2),
        "take_profit_pct": round(abs(take_profit - current_price) / current_price * 100, 2),
        "risk_reward_ratio": round(actual_rr, 2),
    }
```

**Prompt ä¿®æ”¹**:

```python
EXECUTOR_QUALITATIVE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“æ‰§è¡Œä¸“å®¶ã€‚

<é‡è¦>
ä½ åªéœ€è¦æä¾›å®šæ€§åˆ¤æ–­ï¼Œä¸éœ€è¦è®¡ç®—å…·ä½“çš„æ­¢æŸæ­¢ç›ˆä»·æ ¼ã€‚
ç³»ç»Ÿä¼šæ ¹æ®ä½ çš„å®šæ€§åˆ¤æ–­è‡ªåŠ¨è®¡ç®—ç²¾ç¡®çš„é£æ§å‚æ•°ã€‚
</é‡è¦>

<è¾“å‡ºæ ¼å¼>
[å†³ç­–]
action: <æ“ä½œç±»å‹>
confidence: <ç½®ä¿¡åº¦ 0-100>
direction_strength: <strong/moderate/weak>  # æ–¹å‘ä¿¡å·å¼ºåº¦
risk_level: <high/medium/low>  # å½“å‰å¸‚åœºé£é™©ç­‰çº§

[å†³ç­–ç†ç”±]
<ä½ çš„æ¨ç†è¿‡ç¨‹>

[å…³é”®å› ç´ ]
- <å› ç´ 1>
- <å› ç´ 2>
</è¾“å‡ºæ ¼å¼>

<å®šæ€§åˆ¤æ–­è¯´æ˜>
direction_strength (æ–¹å‘å¼ºåº¦):
- strong: å¤šä¸ªæŒ‡æ ‡ + è¶‹åŠ¿ + å½¢æ€ä¸€è‡´çœ‹å¤š/çœ‹ç©º
- moderate: éƒ¨åˆ†ä¿¡å·ä¸€è‡´ï¼Œä½†æœ‰ä¸€äº›å™ªéŸ³
- weak: ä¿¡å·æ··åˆï¼Œæ–¹å‘ä¸æ˜ç¡®

risk_level (é£é™©ç­‰çº§):
- high: é«˜æ³¢åŠ¨ã€ä¸´è¿‘æ”¯æ’‘/é˜»åŠ›ã€èµ„é‡‘è´¹ç‡æç«¯
- medium: æ­£å¸¸å¸‚åœºæ¡ä»¶
- low: ä½æ³¢åŠ¨ã€è¶‹åŠ¿æ˜ç¡®ã€æˆäº¤é‡ç¨³å®š
</å®šæ€§åˆ¤æ–­è¯´æ˜>
"""
```

---

### æ–¹æ¡ˆ 5: æ¨ç†ä¸€è‡´æ€§éªŒè¯ ğŸŸ¡

**ä½ç½®**: `nodes/execution/executor_agent.py`

**å®ç°**:

```python
def verify_reasoning_consistency(
    decision: ExecutorOutputSchema, 
    state: TradingDecisionState
) -> tuple[bool, list[str]]:
    """éªŒè¯ Executor å†³ç­–ä¸å‰åº agent ç»“è®ºçš„é€»è¾‘ä¸€è‡´æ€§"""
    
    violations = []
    
    # è§„åˆ™1: é«˜å¹»è§‰åˆ†ä¸åº”æœ‰é«˜ç½®ä¿¡åº¦å…¥åœº
    hallucination_score = state.get("hallucination_score") or state.get("position_hallucination_score") or 0
    if hallucination_score > 50:
        if decision.action in ["signal_entry_long", "signal_entry_short"]:
            if decision.confidence > 70:
                violations.append(
                    f"High confidence ({decision.confidence}%) with high "
                    f"hallucination ({hallucination_score}%)"
                )
    
    # è§„åˆ™2: Bear èƒœå‡ºä¸åº”åšå¤š
    judge_verdict = state.get("judge_verdict") or state.get("position_judge_verdict")
    if judge_verdict and hasattr(judge_verdict, "winning_argument"):
        if judge_verdict.winning_argument == "bear":
            if decision.action == "signal_entry_long":
                violations.append("Entry long when Bear wins debate")
        elif judge_verdict.winning_argument == "bull":
            if decision.action == "signal_entry_short":
                violations.append("Entry short when Bull wins debate")
    
    # è§„åˆ™3: Judge REJECT ä¸åº”å…¥åœº
    if judge_verdict and hasattr(judge_verdict, "verdict"):
        if judge_verdict.verdict.value == "reject":
            if decision.action in ["signal_entry_long", "signal_entry_short"]:
                violations.append("Entry signal when Judge rejected")
    
    # è§„åˆ™4: æ–¹å‘ä¸å…±è¯†å†²çª
    consensus_dir = state.get("consensus_direction")
    if consensus_dir:
        if str(consensus_dir.value) == "long" and decision.action == "signal_entry_short":
            if decision.confidence > 60:
                violations.append("Entry short against long consensus with high confidence")
        if str(consensus_dir.value) == "short" and decision.action == "signal_entry_long":
            if decision.confidence > 60:
                violations.append("Entry long against short consensus with high confidence")
    
    is_consistent = len(violations) == 0
    
    if not is_consistent:
        logger.warning(f"[ExecutorAgent] Reasoning inconsistencies: {violations}")
    
    return is_consistent, violations
```

---

### æ–¹æ¡ˆ 6: å®Œæ•´å¯è§‚æµ‹æ€§ ğŸŸ¡

**ä½ç½®**: `logging/graph_metrics.py` (æ–°å»º)

**å®ç°**:

```python
from dataclasses import dataclass, field
from typing import List, Dict, Optional
import time
from contextlib import contextmanager

@dataclass
class StageMetrics:
    """å•é˜¶æ®µæ‰§è¡ŒæŒ‡æ ‡"""
    stage_name: str
    duration_ms: float = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    success: bool = True
    error: Optional[str] = None

@dataclass
class GraphExecutionMetrics:
    """å®Œæ•´å›¾æ‰§è¡ŒæŒ‡æ ‡"""
    thread_id: str
    pair: str
    execution_path: str  # "entry" / "position"
    
    # æ€»ä½“æŒ‡æ ‡
    total_duration_ms: float = 0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    estimated_cost_usd: float = 0
    
    # é˜¶æ®µæŒ‡æ ‡
    stages: List[StageMetrics] = field(default_factory=list)
    
    # å¹»è§‰æŒ‡æ ‡
    hallucination_score: float = 0
    false_claims_count: int = 0
    corrected_indicators: List[str] = field(default_factory=list)
    
    # å†³ç­–æŒ‡æ ‡
    final_action: str = ""
    final_confidence: float = 0
    confidence_before_calibration: float = 0
    reasoning_consistency: bool = True
    consistency_violations: List[str] = field(default_factory=list)
    
    def add_stage(self, stage: StageMetrics):
        self.stages.append(stage)
        self.total_duration_ms += stage.duration_ms
        self.total_prompt_tokens += stage.prompt_tokens
        self.total_completion_tokens += stage.completion_tokens
    
    def calculate_cost(self, price_per_1k_prompt=0.0005, price_per_1k_completion=0.0015):
        """ä¼°ç®— API æˆæœ¬"""
        self.estimated_cost_usd = (
            (self.total_prompt_tokens / 1000) * price_per_1k_prompt +
            (self.total_completion_tokens / 1000) * price_per_1k_completion
        )
    
    def to_dict(self) -> dict:
        return {
            "thread_id": self.thread_id,
            "pair": self.pair,
            "execution_path": self.execution_path,
            "total_duration_ms": self.total_duration_ms,
            "total_prompt_tokens": self.total_prompt_tokens,
            "total_completion_tokens": self.total_completion_tokens,
            "estimated_cost_usd": self.estimated_cost_usd,
            "stages": [s.__dict__ for s in self.stages],
            "hallucination_score": self.hallucination_score,
            "false_claims_count": self.false_claims_count,
            "final_action": self.final_action,
            "final_confidence": self.final_confidence,
            "reasoning_consistency": self.reasoning_consistency,
        }

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.current_metrics: Optional[GraphExecutionMetrics] = None
    
    def start_execution(self, thread_id: str, pair: str, execution_path: str):
        self.current_metrics = GraphExecutionMetrics(
            thread_id=thread_id,
            pair=pair,
            execution_path=execution_path
        )
    
    @contextmanager
    def stage(self, stage_name: str):
        """é˜¶æ®µè®¡æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start = time.time()
        stage_metrics = StageMetrics(stage_name=stage_name)
        try:
            yield stage_metrics
        except Exception as e:
            stage_metrics.success = False
            stage_metrics.error = str(e)
            raise
        finally:
            stage_metrics.duration_ms = (time.time() - start) * 1000
            if self.current_metrics:
                self.current_metrics.add_stage(stage_metrics)
    
    def record_hallucination(self, score: float, false_claims: int, corrected: list):
        if self.current_metrics:
            self.current_metrics.hallucination_score = score
            self.current_metrics.false_claims_count = false_claims
            self.current_metrics.corrected_indicators = corrected
    
    def record_decision(self, action: str, confidence: float, consistent: bool, violations: list):
        if self.current_metrics:
            self.current_metrics.final_action = action
            self.current_metrics.final_confidence = confidence
            self.current_metrics.reasoning_consistency = consistent
            self.current_metrics.consistency_violations = violations
    
    def finalize(self) -> GraphExecutionMetrics:
        if self.current_metrics:
            self.current_metrics.calculate_cost()
            return self.current_metrics
        return None
```

---

## ä¼˜å…ˆçº§æ’åº

### ğŸ”´ é«˜ä¼˜å…ˆçº§ (ç«‹å³å®æ–½)

| #   | æ–¹æ¡ˆ                        | é¢„æœŸæ”¶ç›Š              | å®æ–½å¤æ‚åº¦         |
| --- | --------------------------- | --------------------- | ------------------ |
| 1   | **Verification-First (VF)** | å‡†ç¡®ç‡æå‡ (å…è´¹åˆé¤) | ä½ (ä¿®æ”¹ prompt)   |
| 2   | **ç»“æ„åŒ–æ•°æ®æº**            | æ¶ˆé™¤è§£æé”™è¯¯          | ä¸­ (ä¿®æ”¹æ•°æ®æµ)    |
| 3   | **Pydantic éªŒè¯**           | æ¶ˆé™¤æ— æ•ˆè¾“å‡º          | ä¸­ (å¢åŠ éªŒè¯å±‚)    |
| 4   | **LLM æ•°å€¼åˆ†ç¦»**            | æ¶ˆé™¤æ•°å€¼å¹»è§‰          | ä¸­ (é‡æ„ Executor) |

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ (è¿­ä»£ä¼˜åŒ–)

| #   | æ–¹æ¡ˆ                 | é¢„æœŸæ”¶ç›Š       | å®æ–½å¤æ‚åº¦         |
| --- | -------------------- | -------------- | ------------------ |
| 5   | **æ¨ç†ä¸€è‡´æ€§éªŒè¯**   | æ£€æµ‹é€»è¾‘çŸ›ç›¾   | ä½                 |
| 6   | **å®Œæ•´å¯è§‚æµ‹æ€§**     | ç›‘æ§è°ƒä¼˜èƒ½åŠ›   | ä¸­                 |
| 7   | **Self-Consistency** | é«˜é£é™©å†³ç­–éªŒè¯ | ä¸­ (å¤šæ¬¡ API è°ƒç”¨) |

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ (é•¿æœŸä¼˜åŒ–)

| #   | æ–¹æ¡ˆ                    | é¢„æœŸæ”¶ç›Š         | å®æ–½å¤æ‚åº¦ |
| --- | ----------------------- | ---------------- | ---------- |
| 8   | **Post-Executor Judge** | é¢å¤–éªŒè¯å±‚       | ä¸­         |
| 9   | **Self-Refine å¾ªç¯**    | è´¨é‡è¿­ä»£æå‡     | é«˜         |
| 10  | **å¤šè½®è¾©è®º**            | å¤æ‚åœºæ™¯æ·±å…¥åˆ†æ | é«˜         |

---

## å®æ–½è®¡åˆ’

### Phase 1: æ ¸å¿ƒé˜²å¹»è§‰ (1-2å¤©) âœ… å·²å®Œæˆ

- [x] å®ç° Verification-First prompt ç­–ç•¥ âœ…
  - åˆ›å»º `prompts/execution/executor_prompt.py` ä¸­çš„ `EXECUTOR_VF_SYSTEM_PROMPT`
  - å®ç° `build_vf_executor_user_prompt()` å’Œ `build_candidate_answer()`
- [x] åœ¨ `state.py` æ·»åŠ  `verified_indicator_data` å­—æ®µ âœ…
- [x] ä¿®æ”¹ `grounding_node.py` ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–æ•°æ® âœ…
- [x] æ·»åŠ  Pydantic Schema éªŒè¯ âœ…
  - åˆ›å»º `schemas/executor_schemas.py`

### Phase 2: æ•°å€¼åˆ†ç¦» (1å¤©) âœ… å·²å®Œæˆ

- [x] åˆ›å»º `ExecutorQualitativeOutput` Schema âœ…
- [x] å®ç° `calculate_risk_management()` å‡½æ•° âœ…
- [x] ä¿®æ”¹ Executor prompt ä¸ºå®šæ€§è¾“å‡º âœ…
- [x] æµ‹è¯•æ­¢æŸ/æ­¢ç›ˆè®¡ç®—é€»è¾‘ âš ï¸ éœ€è¦é›†æˆæµ‹è¯•

### Phase 3: éªŒè¯ä¸ç›‘æ§ (1å¤©) âœ… å·²å®Œæˆ

- [x] å®ç° `verify_reasoning_consistency()` âœ…
- [x] åˆ›å»º `GraphExecutionMetrics` ç±» âœ…
  - åˆ›å»º `logging/graph_metrics.py`
- [x] é›†æˆæŒ‡æ ‡æ”¶é›†åˆ°ä¸»å›¾æ‰§è¡Œæµç¨‹ âš ï¸ éœ€è¦åœ¨ main_graph.py ä¸­é›†æˆ
- [ ] æ·»åŠ ç›‘æ§ä»ªè¡¨æ¿ (å¯é€‰)

### Phase 4: æµ‹è¯•ä¸è°ƒä¼˜ (æŒç»­)

- [ ] è¿è¡Œé›†æˆæµ‹è¯•
- [ ] ç›‘æ§å¹»è§‰æ£€æµ‹ç‡
- [ ] è°ƒä¼˜é˜ˆå€¼å‚æ•°
- [ ] æ”¶é›†åé¦ˆè¿­ä»£

---

## å‚è€ƒèµ„æ–™

### è®ºæ–‡

1. "Asking LLMs to Verify First is Almost Free Lunch" (arXiv 2511.21734)
2. "Efficient LLM Safety Evaluation through Multi-Agent Debate" (arXiv 2511.06396)
3. "Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference" (arXiv 2511.11306)
4. "LLM-Generated Bayesian Networks for Transparent Trading" (arXiv 2512.01123)
5. "SeSE: Semantic Uncertainty Quantification for Hallucination Detection" (arXiv 2511.16275)
6. "Hybrid Neuro-Symbolic Models for Ethical AI" (arXiv 2511.17644)
7. "A Survey On LLM-as-a-Judge" (Gu, Jiawei et al., 2024)

### è¡Œä¸šå®è·µ

- Amazon Neuro-Symbolic Automated Reasoning
- EY Knowledge Graph Integration
- Glean RAG Architecture
- Spring AI LLM-as-Judge Implementation
- Dynatrace Agent Observability

### å·¥å…·

- Pydantic: ç»“æ„åŒ–è¾“å‡ºéªŒè¯
- TruLens: å¹»è§‰è¿½è¸ª
- LangSmith: LangGraph å¯è§‚æµ‹æ€§

---

> æœ€åæ›´æ–°: 2025-12-06
> ä½œè€…: AI Assistant (åŸºäº Tavily ç ”ç©¶)
