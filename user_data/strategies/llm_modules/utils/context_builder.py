"""
ä¸Šä¸‹æ–‡æ„å»ºå™¨æ¨¡å—
è´Ÿè´£æ„å»ºLLMå†³ç­–æ‰€éœ€çš„å¸‚åœºä¸Šä¸‹æ–‡ï¼ˆé‡æ„ç‰ˆï¼šä½¿ç”¨æ¨¡å—åŒ–ç»„ä»¶ï¼‰
"""
import logging
import math
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
import pandas as pd
from .market_sentiment import MarketSentiment

# å¯¼å…¥æ–°çš„æ¨¡å—åŒ–ç»„ä»¶
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from context.data_formatter import DataFormatter
from context.prompt_builder import PromptBuilder

logger = logging.getLogger(__name__)


class ContextBuilder:
    """LLMä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼ˆé—¨é¢ç±»ï¼Œåè°ƒå„ä¸ªæ¨¡å—ï¼‰"""

    def __init__(self, context_config: Dict[str, Any], historical_query_engine=None, pattern_analyzer=None):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨

        Args:
            context_config: ä¸Šä¸‹æ–‡é…ç½®
            historical_query_engine: å†å²æŸ¥è¯¢å¼•æ“å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            pattern_analyzer: æ¨¡å¼åˆ†æå™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config = context_config
        self.max_tokens = context_config.get("max_context_tokens", 6000)
        self.sentiment = MarketSentiment()  # åˆå§‹åŒ–å¸‚åœºæƒ…ç»ªè·å–å™¨

        # å­¦ä¹ ç³»ç»Ÿç»„ä»¶
        self.historical_query = historical_query_engine
        self.pattern_analyzer = pattern_analyzer
        self.enable_learning = historical_query_engine is not None

        # å…ˆåˆå§‹åŒ–æ–°çš„æ¨¡å—åŒ–ç»„ä»¶ï¼ˆåœ¨ä½¿ç”¨å®ƒä»¬ä¹‹å‰ï¼‰
        self.formatter = DataFormatter()
        self.prompt_builder = PromptBuilder(
            include_timeframe_guidance=context_config.get("include_timeframe_guidance", True)
        )

        self.include_timeframe_guidance = context_config.get(
            "include_timeframe_guidance",
            True
        )
        self.raw_kline_history_points = max(0, context_config.get("raw_kline_history_points", 0))
        self.raw_kline_max_rows = max(
            1,
            context_config.get(
                "raw_kline_max_rows",
                self.raw_kline_history_points or 1
            )
        )
        self.raw_kline_extra_fields = self._ensure_list(
            context_config.get("raw_kline_extra_fields", [])
        )
        self.raw_kline_compact = context_config.get("raw_kline_compact_format", True)
        self.raw_kline_stride = max(1, context_config.get("raw_kline_stride", 1))
        self.indicator_history_points = max(1, context_config.get("indicator_history_points", 20))
        self.indicator_history_lookback = max(
            self.indicator_history_points,
            context_config.get("indicator_history_lookback", 100)
        )
        self.include_multi_timeframe_data = context_config.get("include_multi_timeframe_data", True)
        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨ formatter æ–¹æ³•äº†
        self.multi_timeframe_history = self._normalize_multi_timeframe_config(
            context_config.get("multi_timeframe_history", {})
        ) if self.include_multi_timeframe_data else {}
        self.multi_timeframe_compact = context_config.get(
            "multi_timeframe_compact_format",
            self.raw_kline_compact
        )
        self.multi_timeframe_max_rows = max(
            1,
            context_config.get("multi_timeframe_max_rows", 120)
        )

    def _format_vision_analysis(self, result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è§†è§‰åˆ†æç»“æœä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            result: vision_tools.analyze_image_with_gemini è¿”å›çš„ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not result or not isinstance(result, dict):
            return ""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
        summary = result.get('summary', '')
        if summary.startswith('VISION_CALL_FAILED'):
            return f"âš ï¸ è§†è§‰åˆ†æå¤±è´¥: {summary}"
        
        # æ„å»ºæ ¼å¼åŒ–æ–‡æœ¬
        lines = []
        
        # 1. æ€»ç»“
        if summary:
            lines.append(f"ğŸ“Š **åˆ†ææ€»ç»“**ï¼š{summary}")
            lines.append("")
        
        # 2. è¶‹åŠ¿åˆ¤æ–­
        judgement = result.get('judgement', {})
        if judgement:
            direction_emoji = {
                'up': 'ğŸ“ˆ ä¸Šæ¶¨',
                'down': 'ğŸ“‰ ä¸‹è·Œ',
                'sideways': 'â¡ï¸ æ¨ªç›˜'
            }
            direction = judgement.get('direction', 'unknown')
            confidence = judgement.get('confidence', 0.0)
            
            lines.append(f"ğŸ¯ **è¶‹åŠ¿åˆ¤æ–­**ï¼š{direction_emoji.get(direction, direction)} (ç½®ä¿¡åº¦: {confidence:.1%})")
            
            # è¯æ®åˆ—è¡¨
            evidence = judgement.get('evidence', [])
            if evidence:
                lines.append("   **æ”¯æ’‘è¯æ®**ï¼š")
                for i, item in enumerate(evidence, 1):
                    lines.append(f"   {i}. {item}")
            lines.append("")
        
        # 3. è¯†åˆ«çš„å½¢æ€
        patterns = result.get('patterns', [])
        if patterns:
            lines.append("ğŸ” **è¯†åˆ«çš„Kçº¿å½¢æ€**ï¼š")
            for pattern in patterns:
                name = pattern.get('name', 'Unknown')
                conf = pattern.get('confidence', 0.0)
                lines.append(f"   â€¢ {name} (ç½®ä¿¡åº¦: {conf:.1%})")
            lines.append("")
        
        # 4. é£é™©æç¤º
        risks = result.get('risks', [])
        if risks:
            lines.append("âš ï¸ **é£é™©æç¤º**ï¼š")
            for i, risk in enumerate(risks, 1):
                lines.append(f"   {i}. {risk}")
            lines.append("")
        
        # 5. ä»»åŠ¡ç±»å‹ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
        vision_task = result.get('vision_task', '')
        if vision_task:
            lines.append(f"_ï¼ˆåˆ†æç±»å‹: {vision_task}ï¼‰_")
        
        return "\n".join(lines)

    def build_market_context_with_image(
        self,
        dataframe: pd.DataFrame,
        metadata: Dict[str, Any],
        wallets: Any = None,
        current_trades: Optional[List[Any]] = None,
        exchange: Any = None,
        position_tracker: Any = None,
        market_comparator: Any = None,
        multi_timeframe_data: Optional[Dict[str, pd.DataFrame]] = None,
        chart_image_b64: Optional[str] = None,
        vision_tools: Any = None
    ) -> Dict[str, Any]:
        """
        æ„å»ºå®Œæ•´çš„å¸‚åœºä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å¯é€‰çš„Geminiè§†è§‰åˆ†æï¼‰

        Args:
            dataframe: OHLCVæ•°æ®å’Œæ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
            metadata: äº¤æ˜“å¯¹å…ƒæ•°æ®
            wallets: é’±åŒ…å¯¹è±¡ï¼ˆç”¨äºè·å–è´¦æˆ·ä½™é¢ï¼‰
            current_trades: å½“å‰æ‰€æœ‰æŒä»“åˆ—è¡¨
            exchange: äº¤æ˜“æ‰€å¯¹è±¡ï¼ˆç”¨äºè·å–èµ„é‡‘è´¹ç‡ï¼‰
            position_tracker: PositionTrackerå®ä¾‹ï¼Œæä¾›æŒä»“è¡¨ç°
            market_comparator: MarketStateComparatorå®ä¾‹ï¼Œç”¨äºå¯¹æ¯”
            multi_timeframe_data: å…¶ä»–æ—¶é—´æ¡†æ¶çš„Kçº¿ä¸æŒ‡æ ‡æ•°æ®
            chart_image_b64: å¯é€‰çš„base64ç¼–ç å›¾ç‰‡
            vision_tools: VisionToolså®ä¾‹ï¼Œç”¨äºGeminiè§†è§‰åˆ†æ

        Returns:
            {
                "text_context": str,              # æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è§†è§‰åˆ†æç»“æœï¼‰
                "has_vision_analysis": bool       # æ˜¯å¦åŒ…å«è§†è§‰åˆ†æ
            }
        """
        # æ„å»ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ˆè°ƒç”¨åŸæ–¹æ³•ï¼‰
        text_context = self.build_market_context(
            dataframe=dataframe,
            metadata=metadata,
            wallets=wallets,
            current_trades=current_trades,
            exchange=exchange,
            position_tracker=position_tracker,
            market_comparator=market_comparator,
            multi_timeframe_data=multi_timeframe_data
        )
        
        # å¦‚æœæœ‰å›¾ç‰‡ä¸”æä¾›äº† vision_toolsï¼Œè°ƒç”¨ Gemini åˆ†æ
        vision_analysis = ""
        if chart_image_b64 and vision_tools:
            try:
                logger.info("ğŸ“¸ è°ƒç”¨ Gemini è§†è§‰åˆ†æ...")
                result = vision_tools.analyze_image_with_gemini(
                    image_b64=chart_image_b64,
                    task="trend",  # è¶‹åŠ¿åˆ†æ
                    time_frame=metadata.get('timeframe', '30m'),
                    pair=metadata.get('pair'),  # äº¤æ˜“å¯¹åç§°
                    return_format="json"
                )
                
                # æ ¼å¼åŒ–ç»“æ„åŒ–åˆ†æç»“æœï¼ˆå……åˆ†åˆ©ç”¨ Pydantic æ¨¡å‹è¿”å›çš„æ•°æ®ï¼‰
                vision_analysis = self._format_vision_analysis(result)
                
                logger.info("âœ… Gemini è§†è§‰åˆ†æå®Œæˆ")
                logger.info(f"å‚æ•°: {json.dumps(result, indent=4)}")
                logger.debug(f"è§†è§‰åˆ†æé•¿åº¦: {len(vision_analysis)} å­—ç¬¦")
            except Exception as e:
                logger.error(f"âŒ Gemini è§†è§‰åˆ†æå¤±è´¥: {e}")
                vision_analysis = ""
        
        # å°†è§†è§‰åˆ†æç»“æœæ·»åŠ åˆ°æ–‡æœ¬ä¸Šä¸‹æ–‡
        if vision_analysis:
            text_context += f"\n\n{'='*60}\nã€Kçº¿å›¾è§†è§‰åˆ†æã€‘\n{'='*60}\n{vision_analysis}\n{'='*60}"
        
        # è¿”å›ç»“æ„åŒ–æ•°æ®
        return {
            "text_context": text_context,
            "has_vision_analysis": bool(vision_analysis)
        }
    
    def build_market_context(
        self,
        dataframe: pd.DataFrame,
        metadata: Dict[str, Any],
        wallets: Any = None,
        current_trades: Optional[List[Any]] = None,
        exchange: Any = None,
        position_tracker: Any = None,
        market_comparator: Any = None,
        multi_timeframe_data: Optional[Dict[str, pd.DataFrame]] = None
    ) -> str:
        """
        æ„å»ºå®Œæ•´çš„å¸‚åœºä¸Šä¸‹æ–‡ï¼ˆä¸€æ¬¡æ€§æä¾›æ‰€æœ‰æ•°æ®ï¼‰

        Args:
            dataframe: OHLCVæ•°æ®å’Œæ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
            metadata: äº¤æ˜“å¯¹å…ƒæ•°æ®
            wallets: é’±åŒ…å¯¹è±¡ï¼ˆç”¨äºè·å–è´¦æˆ·ä½™é¢ï¼‰
            current_trades: å½“å‰æ‰€æœ‰æŒä»“åˆ—è¡¨
            exchange: äº¤æ˜“æ‰€å¯¹è±¡ï¼ˆç”¨äºè·å–èµ„é‡‘è´¹ç‡ï¼‰
            position_tracker: PositionTrackerå®ä¾‹ï¼Œæä¾›æŒä»“è¡¨ç°
            market_comparator: MarketStateComparatorå®ä¾‹ï¼Œç”¨äºå¯¹æ¯”
            multi_timeframe_data: å…¶ä»–æ—¶é—´æ¡†æ¶çš„Kçº¿ä¸æŒ‡æ ‡æ•°æ®

        Returns:
            æ ¼å¼åŒ–çš„å®Œæ•´ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        pair = metadata.get('pair', 'UNKNOWN')

        # è·å–æœ€æ–°æ•°æ®
        if dataframe.empty:
            return f"å¸‚åœºæ•°æ®: {pair} - æ— æ•°æ®"

        latest = dataframe.iloc[-1]
        prev = dataframe.iloc[-2] if len(dataframe) > 1 else latest

        context_parts = [
            f"=" * 60,
            f"äº¤æ˜“å¯¹: {pair}",
            f"æ—¶é—´: {latest['date'] if 'date' in latest else datetime.now()}",
            f"=" * 60,
            "",
            "ã€ä»·æ ¼ä¿¡æ¯ã€‘",
            f"  å½“å‰ä»·æ ¼: {latest['close']:.8f}",
            f"  å¼€ç›˜: {latest['open']:.8f}  æœ€é«˜: {latest['high']:.8f}  æœ€ä½: {latest['low']:.8f}",
            f"  æˆäº¤é‡: {latest['volume']:.2f}",
            f"  ä»·æ ¼å˜åŒ–: {((latest['close'] - prev['close']) / prev['close'] * 100):.2f}%",
        ]

        # æš‚å­˜å¸‚åœºæƒ…ç»ªæ•°æ®ï¼Œç¨ååœ¨æœ«å°¾æ·»åŠ 
        sentiment_parts = []
        sentiment_parts.append("")
        sentiment_parts.append("ã€å¸‚åœºæƒ…ç»ªã€‘")
        if exchange:
            try:
                sentiment_data = self.sentiment.get_combined_sentiment(exchange, pair)

                # Fear & Greed Index - æ˜¾ç¤ºå®Œæ•´å†å²
                if sentiment_data.get('fear_greed'):
                    fg = sentiment_data['fear_greed']
                    sentiment_parts.append(f"  ææƒ§ä¸è´ªå©ªæŒ‡æ•°: {fg['value']}/100 ({fg['classification']})")

                    # æ˜¾ç¤º30å¤©å®Œæ•´å†å²ï¼ˆåŸå§‹æ•°æ®ï¼Œä¸åšå¤„ç†ï¼‰
                    if fg.get('history_30d'):
                        sentiment_parts.append("  ")
                        sentiment_parts.append("  å†å²30å¤©ï¼ˆåŸå§‹æ•°æ®ï¼‰ï¼š")
                        history = fg['history_30d']

                        # ç­–ç•¥ï¼šæ˜¾ç¤ºæœ€è¿‘3å¤©æ¯æ—¥ + æ¯å‘¨å…³é”®ç‚¹
                        # æœ€è¿‘3å¤©
                        for i, h in enumerate(history[:3]):
                            if i == 0:
                                time_desc = "ä»Šå¤©"
                            elif i == 1:
                                time_desc = "æ˜¨å¤©"
                            else:
                                time_desc = f"{i}å¤©å‰"
                            sentiment_parts.append(
                                f"    {h['date']} ({time_desc}): {h['value']} ({h['classification']})"
                            )

                        # æ¯å‘¨å…³é”®ç‚¹ï¼šç¬¬7å¤©ã€ç¬¬14å¤©ã€ç¬¬21å¤©ã€ç¬¬30å¤©
                        key_points = [7, 14, 21, 29]  # ç´¢å¼•ä»0å¼€å§‹ï¼Œ29=ç¬¬30å¤©
                        for idx in key_points:
                            if idx < len(history):
                                h = history[idx]
                                days_ago = idx
                                sentiment_parts.append(
                                    f"    {h['date']} ({days_ago}å¤©å‰): {h['value']} ({h['classification']})"
                                )

                # Funding Rate
                if sentiment_data.get('funding_rate'):
                    fr = sentiment_data['funding_rate']
                    sentiment_parts.append("  ")
                    sentiment_parts.append(f"  èµ„é‡‘è´¹ç‡: {fr['rate_pct']:.4f}% ({fr['interpretation']})")

                # Long/Short Ratio - æ˜¾ç¤ºæœ€è¿‘å‡ å¤©çš„è¶‹åŠ¿
                if sentiment_data.get('long_short'):
                    ls = sentiment_data['long_short']
                    sentiment_parts.append("  ")
                    sentiment_parts.append(f"  å¤šç©ºæ¯”: {ls['current_ratio']:.2f} (å¤š{ls['long_pct']:.1f}% / ç©º{ls['short_pct']:.1f}%)")
                    sentiment_parts.append(f"    çŠ¶æ€: {ls['extreme_level']} | è¶‹åŠ¿: {ls['trend']}")

                    # æ˜¾ç¤ºæœ€è¿‘7å¤©çš„å¤šç©ºæ¯”ï¼ˆæ¯12å°æ—¶ä¸€ä¸ªç‚¹ï¼‰
                    if ls.get('history_30d'):
                        history = ls['history_30d']
                        # å–æœ€è¿‘7å¤©ï¼ˆ168å°æ—¶ï¼‰çš„æ•°æ®ï¼Œæ¯12å°æ—¶ä¸€ä¸ªç‚¹ = 14ä¸ªç‚¹
                        recent_7d = history[-168:]
                        sampled = [recent_7d[i] for i in range(0, len(recent_7d), 12)][-14:]

                        if sampled:
                            sentiment_parts.append("    æœ€è¿‘7å¤©å¤šç©ºæ¯”å˜åŒ–ï¼ˆæ¯12å°æ—¶ï¼‰ï¼š")
                            for h in reversed(sampled):  # ä»æ—§åˆ°æ–°
                                from datetime import datetime
                                time_str = datetime.fromtimestamp(h['timestamp'] / 1000).strftime('%m-%d %H:00')
                                sentiment_parts.append(
                                    f"      {time_str}: {h['ratio']:.2f} (å¤š{h['long_pct']:.0f}%/ç©º{h['short_pct']:.0f}%)"
                                )

            except Exception as e:
                logger.error(f"è·å–å¸‚åœºæƒ…ç»ªå¤±è´¥: {e}")


        # è‡ªåŠ¨æå–æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆæ’é™¤åŸºç¡€åˆ—ï¼‰
        excluded_cols = {'date', 'open', 'high', 'low', 'close', 'volume',
                        'enter_long', 'enter_short', 'enter_tag',
                        'exit_long', 'exit_short', 'exit_tag'}

        # æŒ‰æ—¶é—´æ¡†æ¶åˆ†ç»„
        indicators_30m = []  # æ”¹ä¸º30åˆ†é’Ÿ
        indicators_1h = []
        indicators_4h = []
        indicators_1d = []

        for col in latest.index:
            if col in excluded_cols:
                continue

            value = latest[col]
            if pd.isna(value):
                continue

            # åˆ†ç±»æŒ‡æ ‡
            if '_1h' in col:
                indicators_1h.append((col, value))
            elif '_4h' in col:
                indicators_4h.append((col, value))
            elif '_1d' in col:
                indicators_1d.append((col, value))
            else:
                indicators_30m.append((col, value))

        # æŒ‰ç…§ä»å¤§åˆ°å°çš„æ—¶é—´æ¡†æ¶é¡ºåºå‘ˆç°ï¼ˆé«˜æ—¶é—´æ¡†æ¶æ›´é‡è¦ï¼‰

        # è¾“å‡º4å°æ—¶æŒ‡æ ‡ - æœ€é‡è¦ï¼Œå†³å®šå¤§è¶‹åŠ¿
        if self.include_multi_timeframe_data and indicators_4h:
            context_parts.append("")
            context_parts.append("ã€æŠ€æœ¯æŒ‡æ ‡ - 4å°æ—¶ã€‘(å†³å®šå¤§è¶‹åŠ¿)")
            for ind, val in indicators_4h:
                context_parts.append(f"  {ind}: {val:.4f}")

        # è¾“å‡º1å°æ—¶æŒ‡æ ‡ - æ¬¡é‡è¦ï¼Œç¡®è®¤ä¸­æœŸè¶‹åŠ¿
        if self.include_multi_timeframe_data and indicators_1h:
            context_parts.append("")
            context_parts.append("ã€æŠ€æœ¯æŒ‡æ ‡ - 1å°æ—¶ã€‘(ç¡®è®¤ä¸­æœŸè¶‹åŠ¿)")
            for ind, val in indicators_1h:
                context_parts.append(f"  {ind}: {val:.4f}")

        # è¾“å‡º30åˆ†é’ŸæŒ‡æ ‡ - å¯»æ‰¾å…¥åœºæ—¶æœº
        if indicators_30m:
            context_parts.append("")
            context_parts.append("ã€æŠ€æœ¯æŒ‡æ ‡ - 30åˆ†é’Ÿã€‘(å¯»æ‰¾å…¥åœºæ—¶æœº)")
            for ind, val in indicators_30m:
                context_parts.append(f"  {ind}: {val:.4f}")

        # è¾“å‡º1å¤©æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        if self.include_multi_timeframe_data and indicators_1d:
            context_parts.append("")
            context_parts.append("ã€æŠ€æœ¯æŒ‡æ ‡ - 1å¤©ã€‘")
            for ind, val in indicators_1d:
                context_parts.append(f"  {ind}: {val:.4f}")

        # æ·»åŠ è´¦æˆ·ä¿¡æ¯
        if wallets:
            context_parts.append("")
            context_parts.append("ã€è´¦æˆ·ä¿¡æ¯ã€‘")
            try:
                total = wallets.get_total('USDT')
                free = wallets.get_free('USDT')
                used = wallets.get_used('USDT')
                context_parts.extend([
                    f"  æ€»ä½™é¢: {total:.2f} USDT",
                    f"  å¯ç”¨ä½™é¢: {free:.2f} USDT",
                    f"  å·²ç”¨èµ„é‡‘: {used:.2f} USDT",
                    f"  èµ„é‡‘åˆ©ç”¨ç‡: {(used/total*100):.1f}%" if total > 0 else "  èµ„é‡‘åˆ©ç”¨ç‡: 0%"
                ])
            except Exception as e:
                context_parts.append(f"  æ— æ³•è·å–è´¦æˆ·ä¿¡æ¯: {e}")

        # æ·»åŠ æŒä»“ä¿¡æ¯
        context_parts.append("")
        context_parts.append("ã€æŒä»“æƒ…å†µã€‘")
        if not current_trades:
            context_parts.append("  å½“å‰æ— æŒä»“")
        else:
            # ç­›é€‰å½“å‰äº¤æ˜“å¯¹çš„æŒä»“
            pair_trades = [t for t in current_trades if getattr(t, 'pair', '') == pair]

            if not pair_trades:
                context_parts.append(f"  {pair}: æ— æŒä»“")
            else:
                current_price = latest['close']
                for i, trade in enumerate(pair_trades, 1):
                    is_short = getattr(trade, 'is_short', False)
                    open_rate = getattr(trade, 'open_rate', 0)
                    stake = getattr(trade, 'stake_amount', 0)
                    leverage = getattr(trade, 'leverage', 1)
                    enter_tag = getattr(trade, 'enter_tag', '')
                    open_date = getattr(trade, 'open_date', None)

                    # è®¡ç®—å½“å‰ç›ˆäº
                    if is_short:
                        profit_pct = (open_rate - current_price) / open_rate * leverage * 100
                    else:
                        profit_pct = (current_price - open_rate) / open_rate * leverage * 100

                    # è®¡ç®—æŒä»“æ—¶é—´
                    if open_date:
                        from datetime import datetime, timezone
                        if isinstance(open_date, datetime):
                            # freqtradeä½¿ç”¨naive UTCæ—¶é—´ï¼Œæ ¹æ®æ˜¯å¦æœ‰tzinfoé€‰æ‹©å¯¹åº”çš„now
                            now = datetime.utcnow() if open_date.tzinfo is None else datetime.now(timezone.utc)
                            holding_time = now - open_date
                            hours = holding_time.total_seconds() / 3600
                            if hours < 1:
                                time_str = f"{int(hours * 60)}åˆ†é’Ÿ"
                            elif hours < 24:
                                time_str = f"{hours:.1f}å°æ—¶"
                            else:
                                time_str = f"{hours / 24:.1f}å¤©"
                        else:
                            time_str = "æœªçŸ¥"
                    else:
                        time_str = "æœªçŸ¥"

                    # åŸºæœ¬ä¿¡æ¯
                    context_parts.append(f"  æŒä»“#{i}: {'åšç©º' if is_short else 'åšå¤š'} {leverage}xæ æ†")
                    context_parts.append(f"    å¼€ä»“ä»·: {open_rate:.6f}")
                    context_parts.append(f"    å½“å‰ä»·: {current_price:.6f}")
                    context_parts.append(f"    å½“å‰ç›ˆäº: {profit_pct:+.2f}% ({profit_pct * stake / 100:+.2f}U)")
                    context_parts.append(f"    æŒä»“æ—¶é—´: {time_str}")
                    context_parts.append(f"    æŠ•å…¥: {stake:.2f}U")

                    # æ·»åŠ PositionTrackerçš„è¿½è¸ªæ•°æ®
                    if position_tracker:
                        try:
                            trade_id = getattr(trade, 'id', None)
                            if trade_id:
                                metrics = position_tracker.get_position_metrics(trade_id)
                                if metrics:
                                    context_parts.append("")
                                    context_parts.append("    ã€æŒä»“è¿½è¸ªæ•°æ®ã€‘")
                                    context_parts.append(f"      æœ€å¤§æµ®ç›ˆ(MFE): {metrics['max_profit_pct']:+.2f}%")
                                    context_parts.append(f"      æœ€å¤§æµ®äº(MAE): {metrics['max_loss_pct']:+.2f}%")
                                    if metrics['drawdown_from_peak_pct'] < -1:
                                        context_parts.append(f"      ç›ˆåˆ©å›æ’¤: {metrics['drawdown_from_peak_pct']:+.2f}% (ä»å³°å€¼{metrics['max_profit_pct']:+.2f}%)")
                                    context_parts.append(f"      holdæ¬¡æ•°: {metrics['hold_count']}æ¬¡")

                                    # holdæ¨¡å¼è®°å½•ï¼ˆä¸æ·»åŠ è¯„ä»·ï¼‰
                                    hold_pattern = metrics.get('hold_pattern', {})
                                    if hold_pattern.get('pattern') == 'stuck_in_loop':
                                        context_parts.append(f"      è¿ç»­{hold_pattern['repeat_count']}æ¬¡ä½¿ç”¨ç›¸ä¼¼ç†ç”±hold")
                                        context_parts.append(f"      é‡å¤ç†ç”±: \"{hold_pattern['repeated_reason']}\"")
                                    elif hold_pattern.get('pattern') == 'repeated_reasoning':
                                        context_parts.append(f"      ç†ç”±é‡å¤åº¦: {hold_pattern['repeat_count']}/{hold_pattern['total_holds']}")

                                    # æœ€è¿‘å†³ç­–ï¼ˆå®Œæ•´æ˜¾ç¤ºï¼Œä¸æˆªæ–­ï¼‰
                                    if metrics.get('recent_decisions'):
                                        context_parts.append("      æœ€è¿‘3æ¬¡å†³ç­–:")
                                        for d in metrics['recent_decisions'][-3:]:
                                            time_str_short = d['time'].strftime("%H:%M")
                                            context_parts.append(f"        [{time_str_short}] {d['type']}: {d['reason']}")
                        except Exception as e:
                            pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹

                    # å¼€ä»“ç†ç”±ï¼ˆå®Œæ•´æ˜¾ç¤ºï¼Œä¸é™åˆ¶å­—ç¬¦ï¼‰
                    if enter_tag:
                        context_parts.append("")
                        context_parts.append("    å¼€ä»“ç†ç”±:")
                        # åˆ†è¡Œæ˜¾ç¤ºï¼Œå®Œæ•´ä¿ç•™
                        for line in enter_tag.split('\n'):
                            if line.strip():
                                context_parts.append(f"      {line.strip()}")

            # æ˜¾ç¤ºå…¶ä»–äº¤æ˜“å¯¹çš„æŒä»“
            other_trades = [t for t in current_trades if getattr(t, 'pair', '') != pair]
            if other_trades:
                context_parts.append(f"  å…¶ä»–äº¤æ˜“å¯¹æŒä»“æ•°: {len(other_trades)}")

        # æ·»åŠ å¸‚åœºçŠ¶æ€å¯¹æ¯”ï¼ˆå¦‚æœæœ‰æŒä»“ä¸”æä¾›äº†market_comparatorï¼‰
        if current_trades and market_comparator and pair_trades:
            context_parts.append("")
            context_parts.append("=" * 60)
            for trade in pair_trades:
                trade_id = getattr(trade, 'id', None)
                if trade_id:
                    try:
                        # è·å–å½“å‰æŒ‡æ ‡
                        current_indicators = {
                            'atr': latest.get('atr', 0),
                            'rsi': latest.get('rsi', 50),
                            'ema_20': latest.get('ema_20', 0),
                            'ema_50': latest.get('ema_50', 0),
                            'macd': latest.get('macd', 0),
                            'macd_signal': latest.get('macd_signal', 0),
                            'adx': latest.get('adx', 0)
                        }

                        # ç”Ÿæˆå¯¹æ¯”æ–‡æœ¬
                        comparison_text = market_comparator.generate_comparison_text(
                            trade_id=trade_id,
                            current_price=latest['close'],
                            current_indicators=current_indicators
                        )

                        context_parts.append(comparison_text)
                    except Exception as e:
                        pass  # é™é»˜å¤±è´¥
            context_parts.append("=" * 60)

        # æ·»åŠ å…³é”®æŒ‡æ ‡å†å²åºåˆ—ï¼ˆä½¿ç”¨æ–°çš„ DataFormatterï¼‰
        context_parts.append("")
        context_parts.append(f"ã€å…³é”®æŒ‡æ ‡å†å²ï¼ˆæœ€è¿‘{self.indicator_history_points}æ ¹Kçº¿ï¼‰ã€‘")
        indicator_history = self.formatter.get_indicator_history(
            dataframe,
            lookback=self.indicator_history_lookback,
            display_points=self.indicator_history_points
        )
        if indicator_history:
            for ind_name, values in indicator_history.items():
                if values and any(v is not None for v in values):
                    values_str = ", ".join([f"{v}" if v is not None else "N/A" for v in values])
                    context_parts.append(f"  {ind_name}: [{values_str}]")
        else:
            context_parts.append("  æ•°æ®ä¸è¶³")

        # æä¾›åŸå§‹Kçº¿å†å²ï¼ˆä½¿ç”¨æ–°çš„ DataFormatterï¼‰
        if self.raw_kline_history_points > 0:
            raw_history = self.formatter.get_raw_kline_history(
                dataframe,
                self.raw_kline_history_points,
                extra_fields=self.raw_kline_extra_fields,
                compact=self.raw_kline_compact,
                stride=self.raw_kline_stride,
                max_rows=self.raw_kline_max_rows
            )
            raw_rows = raw_history.get('rows', [])
            if raw_rows:
                context_parts.append("")
                header_note = []
                if raw_history.get('header'):
                    header_note.append(f"åˆ—: {raw_history['header']}")
                stride_used = raw_history.get('stride', 1)
                if stride_used > 1:
                    header_note.append(f"æ­¥é•¿:{stride_used}")
                header_text = f"ï¼ˆ{'ï¼Œ'.join(header_note)}ï¼‰" if header_note else ""
                context_parts.append(f"ã€Kçº¿å†å²ï¼ˆæœ€è¿‘{len(raw_rows)}æ ¹ï¼‰ã€‘{header_text}")
                for entry in raw_rows:
                    context_parts.append(f"  {entry}")

        # å¤šæ—¶é—´æ¡†æ¶åŸå§‹æ•°æ®
        if self.multi_timeframe_history and multi_timeframe_data:
            context_parts.append("")
            context_parts.append("ã€å¤šæ—¶é—´æ¡†æ¶Kçº¿æ•°æ®ã€‘")
            for tf, cfg in self.multi_timeframe_history.items():
                tf_df = multi_timeframe_data.get(tf)
                candles = cfg.get('candles', 0)
                if tf_df is None or candles <= 0:
                    continue

                compact_tf = cfg.get('compact', self.multi_timeframe_compact)
                if compact_tf is None:
                    compact_tf = self.multi_timeframe_compact

                stride_tf = cfg.get('stride', 1)
                try:
                    stride_tf = int(stride_tf)
                except (TypeError, ValueError):
                    stride_tf = 1
                stride_tf = max(1, stride_tf)

                tf_history = self.formatter.get_raw_kline_history(
                    tf_df,
                    candles,
                    extra_fields=cfg.get('fields', []),
                    compact=compact_tf,
                    stride=stride_tf,
                    max_rows=cfg.get('max_rows', self.multi_timeframe_max_rows)
                )
                tf_rows = tf_history.get('rows', [])
                if tf_rows:
                    header_note = []
                    if tf_history.get('header'):
                        header_note.append(f"åˆ—: {tf_history['header']}")
                    stride_used = tf_history.get('stride', 1)
                    if stride_used > 1:
                        header_note.append(f"æ­¥é•¿:{stride_used}")
                    header_text = f"ï¼ˆ{'ï¼Œ'.join(header_note)}ï¼‰" if header_note else ""
                    context_parts.append(f"  [{tf}] æœ€è¿‘{len(tf_rows)}æ ¹Kçº¿{header_text}")
                    for entry in tf_rows:
                        context_parts.append(f"    {entry}")

        # åœ¨æœ€åæ·»åŠ å¸‚åœºæƒ…ç»ªå‚è€ƒï¼ˆå¼±åŒ–æ˜¾ç¤ºï¼‰
        context_parts.extend(sentiment_parts)

        # æ·»åŠ å†å²ç»éªŒå’Œæ¨¡å¼åˆ†æï¼ˆè‡ªæˆ‘å­¦ä¹ ç³»ç»Ÿï¼‰
        if self.enable_learning:
            try:
                context_parts.append("")
                # è·å–æœ€è¿‘äº¤æ˜“
                recent_trades_text = self.historical_query.format_recent_trades_for_context(
                    pair=pair,
                    limit=5
                )
                context_parts.append(recent_trades_text)

                # è·å–ç»Ÿè®¡æ‘˜è¦
                context_parts.append("")
                summary_text = self.historical_query.format_pair_summary_for_context(
                    pair=pair,
                    days=30
                )
                context_parts.append(summary_text)

                # è·å–æ¨¡å¼åˆ†æ
                if self.pattern_analyzer:
                    recent_trades = self.historical_query.query_recent_trades(pair=pair, limit=50)
                    if len(recent_trades) >= 5:
                        context_parts.append("")
                        patterns_text = self.pattern_analyzer.format_patterns_for_context(
                            pair=pair,
                            trades=recent_trades
                        )
                        context_parts.append(patterns_text)

            except Exception as e:
                logger.error(f"æ·»åŠ å†å²ç»éªŒå¤±è´¥: {e}")

        context_parts.append("")
        context_parts.append("=" * 60)

        return "\n".join(context_parts)

    def build_position_context(
        self,
        current_trades: List[Any],
        pair: str
    ) -> str:
        """
        æ„å»ºå½“å‰æŒä»“ä¸Šä¸‹æ–‡

        Args:
            current_trades: å½“å‰äº¤æ˜“åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯Tradeå¯¹è±¡æˆ–å­—å…¸ï¼‰
            pair: äº¤æ˜“å¯¹

        Returns:
            æ ¼å¼åŒ–çš„æŒä»“ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not current_trades:
            return f"{pair} å½“å‰æ— æŒä»“"

        # æŸ¥æ‰¾å½“å‰äº¤æ˜“å¯¹çš„æŒä»“ï¼ˆå…¼å®¹dictå’Œå¯¹è±¡ï¼‰
        pair_trades = []
        for t in current_trades:
            t_pair = t.get('pair') if isinstance(t, dict) else getattr(t, 'pair', None)
            if t_pair == pair:
                pair_trades.append(t)

        if not pair_trades:
            return f"{pair} å½“å‰æ— æŒä»“"

        context_parts = [f"\n{'='*50}", f"{pair} ã€æŒä»“è¯¦æƒ…ã€‘", f"{'='*50}"]

        for i, trade in enumerate(pair_trades, 1):
            # å…¼å®¹å­—å…¸å’Œå¯¹è±¡
            if isinstance(trade, dict):
                direction = "åšç©º" if trade.get('side') == 'short' else "åšå¤š"
                open_rate = trade.get('open_rate', 0)
                current_rate = trade.get('current_rate', open_rate)
                amount = trade.get('amount', 0)
                stake_amount = trade.get('stake_amount', 0)
                leverage = trade.get('leverage', 1)
                profit_pct = trade.get('profit_pct', 0)
                profit_abs = stake_amount * profit_pct / 100 if stake_amount else 0
                stop_loss = trade.get('stop_loss')
                duration = trade.get('duration_minutes', 0)

                # è®¡ç®—æŒä»“å¤©æ•°å’Œå°æ—¶
                days = duration // 1440
                hours = (duration % 1440) // 60
                mins = duration % 60

                context_parts.extend([
                    f"\næŒä»“ #{i}:",
                    f"  äº¤æ˜“æ–¹å‘: {direction} {leverage}xæ æ†",
                    f"  å¼€ä»“ä»·æ ¼: {open_rate:.2f}",
                    f"  å½“å‰ä»·æ ¼: {current_rate:.2f}",
                    f"  æŒä»“æ•°é‡: {amount:.4f}",
                    f"  æŠ•å…¥èµ„é‡‘: {stake_amount:.2f} USDT",
                    f"  å½“å‰ç›ˆäº: {profit_pct:.2f}% ({profit_abs:+.2f} USDT)",
                    f"  æŒä»“æ—¶é—´: {days}å¤©{hours}å°æ—¶{mins}åˆ†é’Ÿ",
                ])
            else:
                # å¯¹è±¡æ ¼å¼
                direction = "åšç©º" if getattr(trade, 'is_short', False) else "åšå¤š"
                current_rate = getattr(trade, 'close_rate', None) or getattr(trade, 'open_rate', 0)
                profit_pct = trade.calc_profit_ratio(current_rate) * 100 if hasattr(trade, 'calc_profit_ratio') else 0
                profit_abs = getattr(trade, 'stake_amount', 0) * profit_pct / 100
                leverage = getattr(trade, 'leverage', 1)

                # æŒä»“æ—¶é—´
                from datetime import datetime, timezone
                open_date = getattr(trade, 'open_date', None)
                if open_date:
                    # freqtradeä½¿ç”¨naive UTCæ—¶é—´ï¼Œæ ¹æ®æ˜¯å¦æœ‰tzinfoé€‰æ‹©å¯¹åº”çš„now
                    now = datetime.utcnow() if open_date.tzinfo is None else datetime.now(timezone.utc)
                    duration = (now - open_date).total_seconds() / 60
                    days = int(duration // 1440)
                    hours = int((duration % 1440) // 60)
                    mins = int(duration % 60)
                    duration_str = f"{days}å¤©{hours}å°æ—¶{mins}åˆ†é’Ÿ"
                else:
                    duration_str = "æœªçŸ¥"

                context_parts.extend([
                    f"\næŒä»“ #{i}:",
                    f"  äº¤æ˜“æ–¹å‘: {direction} {leverage}xæ æ†",
                    f"  å¼€ä»“ä»·æ ¼: {trade.open_rate:.2f}",
                    f"  å½“å‰ä»·æ ¼: {current_rate:.2f}",
                    f"  æŒä»“æ•°é‡: {trade.amount:.4f}",
                    f"  æŠ•å…¥èµ„é‡‘: {getattr(trade, 'stake_amount', 0):.2f} USDT",
                    f"  å½“å‰ç›ˆäº: {profit_pct:.2f}% ({profit_abs:+.2f} USDT)",
                    f"  æŒä»“æ—¶é—´: {duration_str}",
                ])

        context_parts.append(f"{'='*50}\n")
        return "\n".join(context_parts)

    def build_system_prompt(self) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯

        Returns:
            ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        parts = [
            "ä½ æ˜¯ä¸“ä¸šçš„åŠ å¯†è´§å¸æ°¸ç»­åˆçº¦äº¤æ˜“å‘˜ã€‚ç›®æ ‡ï¼šé€šè¿‡åšå¤š/åšç©ºæ°¸ç»­åˆçº¦è·åˆ©ã€‚",
            "",
            "ã€æ ¸å¿ƒäº¤æ˜“åŸåˆ™ã€‘",
            "  1. æŠ€æœ¯é¢å’Œä»·æ ¼ä½ç½®æ˜¯å†³ç­–çš„ä¸»è¦ä¾æ®",
            "  2. é£é™©æ”¶ç›Šæ¯”å¿…é¡»â‰¥1.5ï¼Œå¦åˆ™è§‚æœ›",
            "  3. æƒ…ç»ªæŒ‡æ ‡ä»…ä½œä¸ºè¾…åŠ©å‚è€ƒï¼Œä¸åº”å•ç‹¬ä½œä¸ºå¼€ä»“ç†ç”±",
            "  4. è¶‹åŠ¿ä¸æ˜ç¡®æ—¶å®å¯ç­‰å¾…ï¼Œä¸è¦å¼ºè¡Œå¼€ä»“",
            "  5. ä½ æ˜¯è¶‹åŠ¿äº¤æ˜“å‘˜ï¼Œè®©åˆ©æ¶¦å¥”è·‘ï¼Œä¸è¦åšåªèµšèš‚èšè‚‰çš„çŸ­çº¿äº¤æ˜“å‘˜",
            "  6. ä½ çš„å†³ç­–é—´éš”æ˜¯15åˆ†é’Ÿï¼Œä¸‹æ¬¡å†³ç­–åœ¨15åˆ†é’Ÿåï¼Œä¸è¦å¯¹å•Kçº¿çš„æŒ‡æ ‡æ³¢åŠ¨è¿‡åº¦ååº”",
            "",
            "ã€å¸‚åœºç»“æ„ã€‘",
            "  - æ‘†åŠ¨é«˜ç‚¹ï¼ˆswing highï¼‰= é˜»åŠ›ä½ï¼Œä»·æ ¼å¤šæ¬¡åœ¨æ­¤å—é˜»",
            "  - æ‘†åŠ¨ä½ç‚¹ï¼ˆswing lowï¼‰= æ”¯æ’‘ä½ï¼Œä»·æ ¼å¤šæ¬¡åœ¨æ­¤æ­¢è·Œ",
            "  - ä»·æ ¼ä½ç½®ç›¸å¯¹å…³é”®ä½çš„è·ç¦»å†³å®šé£é™©æ”¶ç›Šæ¯”",
        ]

        if self.include_timeframe_guidance:
            parts.extend([
                "",
                "ã€å¤šæ—¶é—´æ¡†æ¶ä¼˜å…ˆçº§ã€‘",
                "  - å¤§å‘¨æœŸ > å°å‘¨æœŸï¼š1d > 4h > 1h > 15m",
                "  - æ¡†æ¶å†²çªæ—¶è·Ÿéšå¤§å‘¨æœŸï¼Œå°å‘¨æœŸä»…ä½œä¸ºå…¥åœºæ—¶æœºå‚è€ƒ",
            ])
        else:
            parts.extend([
                "",
                "ã€è¶‹åŠ¿åˆ¤æ–­ã€‘",
                "  - ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡æä¾›çš„çœŸå®Kçº¿/æŒ‡æ ‡æ¥è¯„ä¼°è¶‹åŠ¿ï¼Œä¸è¦è‡†æµ‹æœªæä¾›çš„æ•°æ®",
                "  - å¦‚æœä¸åŒæ—¶é—´æ¡†æ¶æ•°æ®çŸ›ç›¾ï¼Œä»¥é£é™©æ”¶ç›Šå’Œå…³é”®ä»·ä½ä¸ºå…ˆï¼Œå®å¯è§‚æœ›",
            ])

        parts.extend([
            "",
            "ã€äº¤æ˜“å·¥å…·ï¼ˆå‡½æ•°è°ƒç”¨ï¼‰ã€‘",
            "  ç©ºä»“çŠ¶æ€ï¼š",
            "    â€¢ signal_entry_long  - åšå¤šå¼€ä»“ (limit_price, leverage, stoploss_pct, stake_amount, confidence_score, key_support/resistance, rsi, trend_strength, reason)",
            "    â€¢ signal_entry_short - åšç©ºå¼€ä»“ (å‚æ•°åŒä¸Š)",
            "    â€¢ signal_wait        - è§‚æœ›ä¸å¼€ä»“ (confidence_score, rsi, reason)",
            "  æŒä»“çŠ¶æ€ï¼š",
            "    â€¢ signal_exit     - å¹³ä»“ (limit_price, confidence_score, rsi, reason)",
            "    â€¢ adjust_position - åŠ /å‡ä»“ (adjustment_pct, limit_price, confidence_score, key_support/resistance, reason)",
            "    â€¢ signal_hold     - ç»´æŒæŒä»“ (confidence_score, rsi, reason)",
            "  æ³¨æ„ï¼šå…ˆç¡®è®¤ã€æŒä»“æƒ…å†µã€‘ï¼Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°åç«‹å³åœæ­¢",
            "",
            "ã€å…¥åœºå†³ç­–åˆ†æé¡ºåºã€‘",
            "  ç¬¬ä¸€æ­¥ï¼šä»·æ ¼ä½ç½®åˆ†æ",
            "    â€¢ å½“å‰ä»·æ ¼ç›¸å¯¹æ”¯æ’‘/é˜»åŠ›çš„ä½ç½®",
            "    â€¢ è·ç¦»å…³é”®ä½çš„ç™¾åˆ†æ¯”ï¼ˆè‡³å°‘2-3%ç©ºé—´ï¼‰",
            "    â€¢ ä»·æ ¼æ˜¯å¦å¤„äºæŠ€æœ¯å½¢æ€çš„å…³é”®ç‚¹ä½",
            "  ç¬¬äºŒæ­¥ï¼šæŠ€æœ¯æŒ‡æ ‡ç¡®è®¤",
        ])

        if self.include_timeframe_guidance:
            parts.append("    â€¢ å¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿æ˜¯å¦å¯¹é½ï¼ˆå¤§å‘¨æœŸ > å°å‘¨æœŸï¼‰")
        else:
            parts.append("    â€¢ ä¾æ®æä¾›çš„Kçº¿å’ŒæŒ‡æ ‡æ¨æ–­è¶‹åŠ¿æ–¹å‘")

        parts.extend([
            "    â€¢ RSIæ˜¯å¦å¤„äºåˆç†åŒºé—´ï¼ˆè¶…ä¹°>70ï¼Œè¶…å–<30ï¼‰",
            "    â€¢ MACD/EMAç­‰è¶‹åŠ¿æŒ‡æ ‡æ˜¯å¦æ”¯æŒ",
            "    â€¢ å¤šä¸ªæŒ‡æ ‡æ˜¯å¦å…±æŒ¯",
            "  ç¬¬ä¸‰æ­¥ï¼šé£é™©æ”¶ç›Šè¯„ä¼°",
            "    â€¢ è®¡ç®—é£é™©è·ç¦»ï¼ˆåˆ°å…³é”®æ”¯æ’‘/é˜»åŠ›ï¼‰å’Œç›®æ ‡è·ç¦»",
            "    â€¢ é£é™©æ”¶ç›Šæ¯”å¿…é¡»â‰¥1.5",
            "    â€¢ é£é™©æ”¶ç›Šæ¯”<1.5æ—¶å¿…é¡»é€‰æ‹©signal_wait",
            "  ç¬¬å››æ­¥ï¼šè¾…åŠ©å‚è€ƒï¼ˆå¯é€‰ï¼‰",
            "    â€¢ æƒ…ç»ªæŒ‡æ ‡æ˜¯å¦ä¸æŠ€æœ¯é¢ä¸€è‡´",
            "    â€¢ æˆäº¤é‡æ˜¯å¦é…åˆ",
            "  å†³ç­–ï¼šå‰ä¸‰æ­¥éƒ½æ»¡è¶³æ‰è€ƒè™‘å¼€ä»“ï¼Œä»»ä¸€æ­¥ä¸æ»¡è¶³åˆ™è§‚æœ›",
            "",
            "ã€å¸‚åœºæƒ…ç»ªå‚è€ƒã€‘",
            "  - ä»…ä½œä¸ºè¾…åŠ©å‚è€ƒï¼Œä¸åº”ä½œä¸ºä¸»è¦å¼€ä»“ä¾æ®",
            "  - æç«¯æƒ…ç»ªå¯æŒç»­æ•°å‘¨ç”šè‡³æ•°æœˆ",
            "  - å¼ºè¶‹åŠ¿ä¸­å•è¾¹æƒ…ç»ªæ˜¯å¸¸æ€",
            "",
            "ã€å¹³ä»“è€ƒè™‘å› ç´ ã€‘",
        ])

        if self.include_timeframe_guidance:
            parts.append("  - å…³é”®æ”¯æ’‘/é˜»åŠ›çš„çªç ´æƒ…å†µï¼Œå¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿")
        else:
            parts.append("  - å…³é”®æ”¯æ’‘/é˜»åŠ›çš„çªç ´æƒ…å†µï¼Œå®é™…èµ°åŠ¿ä¸å¼€ä»“é€»è¾‘çš„å¯¹æ¯”")

        parts.extend([
            "  - å¼€ä»“é€»è¾‘æ˜¯å¦ä»ç„¶æˆç«‹ï¼ˆä»·æ ¼æ˜¯å¦è·Œç ´å…³é”®æ”¯æ’‘/çªç ´é˜»åŠ›ï¼‰",
            "  - è¶‹åŠ¿æ˜¯å¦å‘ç”Ÿå®è´¨æ€§åè½¬ï¼ˆä¸æ˜¯æŒ‡æ ‡æ³¢åŠ¨ï¼Œè€Œæ˜¯ä»·æ ¼ç»“æ„æ”¹å˜ï¼‰",
            "  - ç›ˆåˆ©æ˜¯å¦æ¥è¿‘é¢„è®¾ç›®æ ‡",
            "  é‡è¦æé†’ï¼š",
            "    â€¢ ä½ 15åˆ†é’Ÿæ‰èƒ½å†³ç­–ä¸€æ¬¡ï¼ŒADXä¸‹é™ã€MACDè½¬è´Ÿç­‰å•Kçº¿æŒ‡æ ‡å˜åŒ–æ˜¯æ­£å¸¸æ³¢åŠ¨",
            "    â€¢ åªæœ‰ä»·æ ¼è·Œç ´EMA20/å…³é”®æ”¯æ’‘ä½ï¼Œæ‰æ˜¯è¶‹åŠ¿åè½¬çš„ä¿¡å·",
            "    â€¢ ä¸è¦å› ä¸ºæŒ‡æ ‡çŸ­æœŸæ³¢åŠ¨å°±ææ…Œå¹³ä»“ï¼Œè®©åˆ©æ¶¦å¥”è·‘",
            "",
            "ã€æŒä»“ç®¡ç†è€ƒè™‘ã€‘",
            "  - å¤§è¶‹åŠ¿æ˜¯å¦ä»ç„¶æˆç«‹ï¼ˆä»·æ ¼ä¸EMA20çš„ä½ç½®å…³ç³»ï¼‰",
            "  - ç›ˆåˆ©æ˜¯å¦æ¥è¿‘é¢„è®¾ç›®æ ‡ï¼ˆè‡³å°‘70%ä»¥ä¸Šï¼‰",
            "  - ç»™è¶‹åŠ¿æ—¶é—´å‘å±•ï¼Œ15åˆ†é’ŸKçº¿éœ€è¦æ›´é•¿çš„æŒä»“æ—¶é—´",
            "  - è®©åˆ©æ¶¦å¥”è·‘ï¼Œä¸è¦èµšä¸€ç‚¹ç‚¹å°±è·‘",
            "",
            "ã€è®¢å•ç±»å‹ã€‘",
            "  - Limit Orderï¼šåšå¤šlimit<å½“å‰ä»·ï¼Œåšç©ºlimit>å½“å‰ä»·ï¼ˆç­‰å¾…æ›´å¥½ä»·æ ¼ï¼‰",
            "  - Market Orderï¼šåšå¤šlimitâ‰¥å½“å‰ä»·ï¼Œåšç©ºlimitâ‰¤å½“å‰ä»·ï¼ˆç«‹å³å…¥åœºï¼‰",
            "  é€‰æ‹©åŸåˆ™ï¼š",
            "    â€¢ è¶‹åŠ¿æ˜ç¡®+é‡èƒ½æ”¾å¤§+æŠ€æœ¯çªç ´ â†’ å¸‚ä»·å•ç«‹å³å…¥åœºï¼ˆé”™è¿‡æœºä¼šæˆæœ¬é«˜ï¼‰",
            "    â€¢ éœ‡è¡è¡Œæƒ…+æ¥è¿‘æ”¯æ’‘é˜»åŠ› â†’ é™ä»·å•ç­‰å¾…æ›´å¥½ä»·æ ¼",
            "    â€¢ å½“å†³å®šå¼€ä»“æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘å¸‚ä»·å•ï¼Œé¿å…å› ç­‰å¾…è€Œé”™è¿‡è¡Œæƒ…",
            "",
            "ã€é£é™©æ”¶ç›Šæ¯”è®¡ç®—ã€‘",
            "  - é£é™©è·ç¦» = ä»·æ ¼åˆ°å…³é”®æ”¯æ’‘/é˜»åŠ›çš„è·ç¦»ç™¾åˆ†æ¯”",
            "  - ç›®æ ‡è·ç¦» = ä»·æ ¼åˆ°ç›®æ ‡ä½çš„è·ç¦»ç™¾åˆ†æ¯”",
            "  - é£é™©æ”¶ç›Šæ¯” = ç›®æ ‡è·ç¦» / é£é™©è·ç¦»",
            "  - é£é™©æ”¶ç›Šæ¯”å¿…é¡»â‰¥1.5ï¼Œå¦åˆ™å¿…é¡»è°ƒç”¨signal_wait",
            "",
            "ã€æ æ†é€‰æ‹©è€ƒè™‘å› ç´ ã€‘",
            "  - å¸‚åœºæ³¢åŠ¨ï¼ˆATRå æ¯”ï¼‰",
            "  - ä¿¡å·è´¨é‡ï¼ˆå¤šä¸ªæŒ‡æ ‡æ˜¯å¦ä¸€è‡´ï¼‰",
            "  - è¶‹åŠ¿å¼ºåº¦ï¼ˆADXå€¼ï¼‰",
            "  - ä»·æ ¼ä½ç½®ï¼ˆæ˜¯å¦åœ¨å…³é”®ä½é™„è¿‘ï¼‰",
            "  - è´¦æˆ·å¯ç”¨ä½™é¢",
            "",
            "ã€å†³ç­–æ£€æŸ¥æ¸…å•ã€‘",
            "  âœ“ æŠ€æœ¯é¢åˆ†æå®Œæ•´ï¼ˆä»·æ ¼ä½ç½®+æŒ‡æ ‡ç¡®è®¤ï¼‰",
            "  âœ“ é£é™©æ”¶ç›Šæ¯”â‰¥1.5",
            "  âœ“ è¶‹åŠ¿æ–¹å‘æ˜ç¡®",
            "  âœ“ å…³é”®æ”¯æ’‘/é˜»åŠ›ä½æ˜ç¡®",
            "  å¦‚ä»»ä¸€é¡¹ä¸æ»¡è¶³ï¼Œè°ƒç”¨signal_waitå¹¶è¯´æ˜ç†ç”±",
            "",
            "ç»¼åˆåˆ†æåç‹¬ç«‹åˆ¤æ–­ï¼Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°ååœæ­¢ã€‚",
        ])

        return "\n".join(parts)

    def build_entry_system_prompt(self) -> str:
        """
        æ„å»ºå¼€ä»“å†³ç­–ä¸“ç”¨ç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨æ–°çš„ PromptBuilderï¼‰

        Returns:
            å¼€ä»“å†³ç­–ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        return self.prompt_builder.build_entry_prompt()

    def build_position_system_prompt(self) -> str:
        """
        æ„å»ºæŒä»“ç®¡ç†ç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨æ–°çš„ PromptBuilderï¼‰

        Returns:
            æŒä»“ç®¡ç†ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        return self.prompt_builder.build_position_prompt()

    def _build_entry_system_prompt_old(self) -> str:
        """
        æ—§ç‰ˆå¼€ä»“æç¤ºè¯ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå‚è€ƒï¼‰

        Returns:
            å¼€ä»“å†³ç­–ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        parts = [
            "ä½ æ˜¯ä¸“ä¸šçš„åŠ å¯†è´§å¸è¶‹åŠ¿äº¤æ˜“å‘˜ã€‚ä»»åŠ¡ï¼šè¯„ä¼°æ˜¯å¦å¼€ä»“ã€‚",
            "",
            "ã€æ ¸å¿ƒç†å¿µã€‘",
            "",
            "è¶‹åŠ¿äº¤æ˜“çš„æœ¬è´¨ï¼šè¯†åˆ«è¶‹åŠ¿æ–¹å‘å¹¶é¡ºåŠ¿è€Œä¸ºã€‚",
            "å¤§å‘¨æœŸå†³å®šæ–¹å‘ï¼Œå°å‘¨æœŸå¯»æ‰¾å…¥åœºæ—¶æœºã€‚",
            "å…¥åœºæ—¶æœºæœ‰å¤šç§å½¢å¼ï¼šå›è°ƒã€çªç ´ã€å»¶ç»­ã€‚",
            "å¸‚åœºæ˜¯åŠ¨æ€çš„ï¼Œè¦çµæ´»åº”å¯¹ï¼Œä¸è¦æœºæ¢°æ‰§è¡Œè§„åˆ™ã€‚",
            "",
            "ã€å¤šæ—¶é—´æ¡†æ¶åˆ†æã€‘",
            "",
            "æŒ‰é¡ºåºåˆ†æï¼Œä½†è¦ç»¼åˆåˆ¤æ–­ï¼š",
            "",
            "ç¬¬ä¸€æ­¥ï¼šçœ‹4å°æ—¶å›¾ - ç¡®å®šä¸»è¦æ–¹å‘",
            "  EMAæ’åˆ—æ˜¾ç¤ºè¶‹åŠ¿ï¼š",
            "    EMA20 > EMA50 > EMA100ï¼šä¸Šæ¶¨è¶‹åŠ¿ï¼Œåå‘åšå¤š",
            "    EMA20 < EMA50 < EMA100ï¼šä¸‹è·Œè¶‹åŠ¿ï¼Œåå‘åšç©º",
            "    EMAäº¤ç»‡ï¼šéœ‡è¡ï¼Œè°¨æ…",
            "  MACDæŸ±çŠ¶å›¾æ˜¾ç¤ºåŠ¨èƒ½ï¼š",
            "    æŒç»­ä¸ºæ­£ï¼šå¤šå¤´åŠ¨èƒ½",
            "    æŒç»­ä¸ºè´Ÿï¼šç©ºå¤´åŠ¨èƒ½",
            "    é¢‘ç¹ç¿»è½¬ï¼šæ— æ–¹å‘ï¼Œè§‚å¯Ÿ",
            "",
            "ç¬¬äºŒæ­¥ï¼šçœ‹1å°æ—¶å›¾ - ç¡®è®¤æˆ–è¯†åˆ«å›è°ƒ",
            "  ç†æƒ³ï¼š1å°æ—¶å’Œ4å°æ—¶æ–¹å‘ä¸€è‡´",
            "  å¦‚æœç›¸åï¼šå¯èƒ½æ˜¯å¤§è¶‹åŠ¿ä¸­çš„å›è°ƒï¼ˆæœºä¼šï¼‰",
            "  å¦‚æœé•¿æœŸèƒŒç¦»ï¼šå¤§è¶‹åŠ¿å¯èƒ½åœ¨æ”¹å˜",
            "",
            "ç¬¬ä¸‰æ­¥ï¼šçœ‹30åˆ†é’Ÿå›¾ - è¯†åˆ«å…¥åœºæ—¶æœº",
            "  ç¡®è®¤è¶‹åŠ¿æ–¹å‘åï¼Œå¯»æ‰¾å…¥åœºç‚¹ï¼š",
            "    å›è°ƒåçš„åå¼¹ / åå¼¹åçš„ä¸‹è·Œ",
            "    å…³é”®ä½çªç ´åçš„å»¶ç»­",
            "    è¶‹åŠ¿åŠ é€Ÿä¸­çš„è·Ÿéš",
            "",
            "æ³¨æ„ï¼š",
            "  é€†åŠ¿äº¤æ˜“é£é™©æé«˜ï¼ˆå¦‚4å°æ—¶ç©ºå¤´æ—¶åšå¤šï¼‰",
            "  ä½†ä¹Ÿè¦è¯†åˆ«è¶‹åŠ¿åè½¬ï¼Œä¸è¦ç›²ç›®ç›¸ä¿¡å¤§å‘¨æœŸ",
            "  30åˆ†é’Ÿä¿¡å·å•ç‹¬ä¸å¤Ÿï¼Œä½†ç»“åˆå…¶ä»–ç»´åº¦å°±æœ‰æ•ˆ",
            "",
            "ã€æŠ€æœ¯æŒ‡æ ‡ç†è§£ã€‘",
            "",
            "EMA - è¶‹åŠ¿çš„éª¨æ¶ï¼š",
            "  ä¸‰çº¿æ’åˆ—æ˜¾ç¤ºè¶‹åŠ¿åŸºç¡€ï¼š",
            "    å¤šå¤´ï¼ˆEMA20>50>100ï¼‰ï¼šä¸Šæ¶¨è¶‹åŠ¿ä¿¡å·",
            "    ç©ºå¤´ï¼ˆEMA20<50<100ï¼‰ï¼šä¸‹è·Œè¶‹åŠ¿ä¿¡å·",
            "    æ’åˆ—è¶Šæ¸…æ™° = è¶‹åŠ¿è¶Šå¼º",
            "  ",
            "  å¤šç§å…¥åœºæ—¶æœºæ–¹å¼ï¼š",
            "    ç»å…¸ï¼šå›è°ƒåˆ°EMA20åå¼¹ï¼ˆç¨³å¥ï¼Œé€‚ç”¨äºå›è°ƒå…¥åœºï¼‰",
            "    æ¿€è¿›ï¼šä»·æ ¼åœ¨EMA20ä¸Šæ–¹å¥”è·‘æ—¶å…¥åœºï¼ˆé€‚ç”¨äºå¼ºè¶‹åŠ¿ï¼‰",
            "    çªç ´ï¼šä»·æ ¼çªç ´å¹¶ç«™ç¨³EMA20åå…¥åœºï¼ˆé€‚ç”¨äºè¶‹åŠ¿å¯åŠ¨ï¼‰",
            "  ä¸è¦æ­»ç›¯EMA20 - æœ‰æ—¶EMA50ä¹Ÿæ˜¯å¼ºæ”¯æ’‘",
            "",
            "MACD - åŠ¨èƒ½æ¸©åº¦è®¡ï¼š",
            "  å…³æ³¨æŸ±çŠ¶å›¾ï¼ˆhistï¼‰ï¼Œå®ƒåæ˜ åŠ¨èƒ½å˜åŒ–ã€‚",
            "  ",
            "  å…¥åœºä¿¡å·ï¼š",
            "    æŸ±çŠ¶å›¾ä»è´Ÿè½¬æ­£ï¼šå¤šå¤´åŠ¨èƒ½æ¢å¤ï¼ˆç»å…¸å›è°ƒä¹°ç‚¹ï¼‰",
            "    æŸ±çŠ¶å›¾æ‰©å¤§ï¼šè¶‹åŠ¿åŠ é€Ÿï¼ˆå¯ä»¥è¿½ï¼‰",
            "    æŸ±çŠ¶å›¾ä¸ºè´Ÿä½†æ”¶çª„ï¼šåŠ¨èƒ½è¡°ç«­ï¼Œå¯èƒ½åè½¬",
            "  ",
            "  ä¸è¦æœºæ¢°ç­‰MACDè½¬æ­£ - æœ‰æ—¶è¡Œæƒ…æå‰å¯åŠ¨ã€‚",
            "",
            "RSI - è¶…ä¹°è¶…å–å‚è€ƒï¼š",
            "  è¾…åŠ©æŒ‡æ ‡ï¼Œä¸æ˜¯ä¸»è¦ä¾æ®ã€‚",
            "  ",
            "  ç†è§£ï¼š",
            "    ä»30-40ä¸Šå‡ï¼šå›è°ƒå¯èƒ½ç»“æŸ",
            "    ä»60-70ä¸‹é™ï¼šåå¼¹å¯èƒ½ç»“æŸ",
            "    å¼ºè¶‹åŠ¿ä¸­RSIå¯ä»¥é•¿æœŸ>70æˆ–<30",
            "  ",
            "  ä¸è¦å•çº¯å› ä¸ºRSIè¶…ä¹°å°±åšç©ºï¼Œæˆ–è¶…å–å°±åšå¤šã€‚",
            "",
            "å¸ƒæ—å¸¦ - æ³¢åŠ¨ç‡è¾¹ç•Œï¼š",
            "  è¶‹åŠ¿å¸‚åœºï¼šä»·æ ¼æ²¿ä¸Šè½¨ï¼ˆå¤šå¤´ï¼‰æˆ–ä¸‹è½¨ï¼ˆç©ºå¤´ï¼‰è¿è¡Œ",
            "  éœ‡è¡å¸‚åœºï¼šä»·æ ¼åœ¨å¸¦å†…æ¥å›",
            "  çªç ´å¸‚åœºï¼šä»·æ ¼çªç ´ä¸Šè½¨å¹¶ç»§ç»­",
            "",
            "ADX - è¶‹åŠ¿å¼ºåº¦è¡¨ï¼š",
            "  ç†è§£ADXæ•°å€¼ï¼š",
            "    >30ï¼šå¼ºè¶‹åŠ¿ï¼Œé€‚åˆè¶‹åŠ¿äº¤æ˜“",
            "    20-30ï¼šä¸­ç­‰è¶‹åŠ¿ï¼Œéœ€è¦æ›´å¤šç¡®è®¤",
            "    <20ï¼šå¼±è¶‹åŠ¿æˆ–éœ‡è¡ï¼Œè°¨æ…",
            "  ",
            "  ä¸è¦æŠŠADXå½“ç¡¬é—¨æ§›ï¼š",
            "    ADX 23å¯èƒ½æ˜¯å¥½è¶‹åŠ¿çš„å¼€å§‹",
            "    ADXä»15å‡åˆ°23æ„å‘³ç€è¶‹åŠ¿åœ¨å¢å¼º",
            "    ç»“åˆå…¶ä»–æŒ‡æ ‡ç»¼åˆåˆ¤æ–­",
            "",
            "ã€æˆäº¤é‡ç†è§£ã€‘",
            "",
            "æˆäº¤é‡æ˜¯ç¡®è®¤å·¥å…·ï¼š",
            "  çªç ´æ—¶æˆäº¤é‡æ”¾å¤§ï¼šçªç ´å¯èƒ½æœ‰æ•ˆ",
            "  çªç ´æ—¶æˆäº¤é‡èç¼©ï¼šçªç ´å¯èƒ½å‡",
            "  ä¸è¦æœºæ¢°è¦æ±‚'1.5å€æ”¾é‡' - å¸‚åœºæ˜¯åŠ¨æ€çš„",
            "",
            "ã€å¸‚åœºæƒ…ç»ªä½¿ç”¨ã€‘",
            "",
            "æƒ…ç»ªæ˜¯å‚è€ƒï¼Œä¸æ˜¯å†³å®šæ€§å› ç´ ã€‚",
            "",
            "å¤šç©ºæ¯”ï¼š",
            "  æç«¯æ¯”å€¼ï¼ˆ>2.5æˆ–<0.4ï¼‰ï¼šå¸‚åœºæƒ…ç»ªåå‘ä¸€è¾¹",
            "  å¯èƒ½å«ä¹‰ï¼š",
            "    å¼ºè¶‹åŠ¿ä¸­çš„æ­£å¸¸ç°è±¡ï¼ˆç»§ç»­è·Ÿéšï¼‰",
            "    è¶‹åŠ¿æœ«ç«¯çš„åè½¬ä¿¡å·ï¼ˆéœ€ä»·æ ¼ç¡®è®¤ï¼‰",
            "  ä¸è¦å•çº¯åŸºäºæ¯”å€¼å¼€ä»“ï¼Œç»“åˆä»·æ ¼è¡Œä¸º",
            "",
            "èµ„é‡‘è´¹ç‡ï¼š",
            "  æç«¯è´¹ç‡åæ˜ æç«¯æƒ…ç»ª",
            "  ä½†å¼ºè¶‹åŠ¿ä¸­è´¹ç‡å¯é•¿æœŸæç«¯",
            "  ä½œä¸ºå‚è€ƒï¼Œä¸æ˜¯ç¡¬ä¿¡å·",
            "",
            "ã€å¤šç§å…¥åœºæ–¹å¼ã€‘",
            "",
            "è¶‹åŠ¿äº¤æ˜“æœ‰å¤šç§å…¥åœºæ—¶æœº - è¦çµæ´»ï¼š",
            "",
            "æ–¹å¼1 - å›è°ƒå…¥åœºï¼ˆç¨³å¥å‹ï¼‰ï¼š",
            "  ä½•æ—¶ï¼šè¶‹åŠ¿ç¡®ç«‹ï¼Œç­‰å¾…å›è°ƒ",
            "  ä¿¡å·ï¼šä»·æ ¼å›è°ƒåˆ°EMA20/50ï¼ŒMACDæŸ±çŠ¶å›¾æ”¶çª„åè½¬å‘",
            "  ä¼˜ç‚¹ï¼šé£é™©ä½ï¼Œå…¥åœºä½ç½®å¥½",
            "  ç¼ºç‚¹ï¼šå¯èƒ½æ²¡æœ‰å›è°ƒ",
            "",
            "æ–¹å¼2 - çªç ´è¿½å…¥ï¼ˆæ¿€è¿›å‹ï¼‰ï¼š",
            "  ä½•æ—¶ï¼šä»·æ ¼çªç ´å…³é”®æ”¯æ’‘/é˜»åŠ›",
            "  ä¿¡å·ï¼šçªç ´åç«™ç¨³ï¼Œæˆäº¤é‡æ”¾å¤§ï¼ŒADXä¸Šå‡",
            "  ä¼˜ç‚¹ï¼šä¸ä¼šé”™è¿‡å¼ºåŠ²èµ°åŠ¿",
            "  ç¼ºç‚¹ï¼šå…¥åœºä½ç½®å¯èƒ½ä¸æ˜¯æœ€ä¼˜",
            "",
            "æ–¹å¼3 - è¶‹åŠ¿å»¶ç»­ï¼ˆè·Ÿéšå‹ï¼‰ï¼š",
            "  ä½•æ—¶ï¼šè¶‹åŠ¿æ˜ç¡®ï¼Œä»·æ ¼æ²¿EMAè¿è¡Œ",
            "  ä¿¡å·ï¼šä»·æ ¼åœ¨EMA20ä¸Šæ–¹ï¼ŒMACDæŒç»­ä¸ºæ­£ï¼ŒADX>25",
            "  ä¼˜ç‚¹ï¼šæ­ä¸Šå¼ºè¶‹åŠ¿",
            "  ç¼ºç‚¹ï¼šå…¥åœºç‚¹ä¸æ˜¯æœ€ä¼˜",
            "",
            "æ–¹å¼4 - è¶‹åŠ¿åè½¬ï¼ˆåå‘å‹ï¼‰ï¼š",
            "  ä½•æ—¶ï¼šè¯†åˆ«è¶‹åŠ¿åè½¬",
            "  ä¿¡å·ï¼šEMAè½¬å‘ï¼Œä»·æ ¼çªç ´EMAï¼Œç»“æ„æ”¹å˜",
            "  ä¼˜ç‚¹ï¼šæŠ“ä½å¤§è¶‹åŠ¿å¼€å§‹",
            "  ç¼ºç‚¹ï¼šéš¾åº¦é«˜ï¼Œå®¹æ˜“é”™",
            "",
            "æ ¹æ®å½“å‰å¸‚åœºçŠ¶æ€é€‰æ‹©åˆé€‚æ–¹å¼ï¼Œä¸è¦å›ºå®ˆä¸€ç§ã€‚",
            "",
            "ã€å¼€ä»“å†³ç­–æ¡†æ¶ã€‘",
            "",
            "ç»¼åˆè¯„ä¼°ï¼Œä¸æ˜¯æœºæ¢°æ‰§è¡Œï¼š",
            "",
            "æ ¸å¿ƒé—®é¢˜ï¼š",
            "  1. è¶‹åŠ¿æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆçœ‹4å°æ—¶EMAæ’åˆ—ï¼‰",
            "  2. è¶‹åŠ¿å¼ºåº¦å¦‚ä½•ï¼Ÿï¼ˆADXã€MACDæŒç»­æ€§ï¼‰",
            "  3. å½“å‰å¤„äºè¶‹åŠ¿çš„ä»€ä¹ˆé˜¶æ®µï¼Ÿï¼ˆå¯åŠ¨ã€å‘å±•ã€è¡°ç«­ï¼‰",
            "  4. æœ‰ä»€ä¹ˆå…¥åœºæ—¶æœºï¼Ÿï¼ˆå›è°ƒã€çªç ´ã€å»¶ç»­ã€åè½¬ï¼‰",
            "  5. é£é™©æ”¶ç›Šæ¯”å¦‚ä½•ï¼Ÿï¼ˆå…³é”®æ”¯æ’‘/é˜»åŠ›ä½ã€ç›®æ ‡ä½ï¼‰",
            "",
            "ç»¼åˆåˆ¤æ–­ï¼š",
            "  å¤šä¸ªç»´åº¦éƒ½æŒ‡å‘åŒä¸€æ–¹å‘ = é«˜ç½®ä¿¡åº¦ï¼Œå¯ä»¥å¼€ä»“",
            "  éƒ¨åˆ†ç»´åº¦ç¡®è®¤ï¼Œéƒ¨åˆ†çŸ›ç›¾ = ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œè°¨æ…å¼€ä»“æˆ–ç­‰å¾…",
            "  ç»´åº¦å†²çªä¸¥é‡ = ä½ç½®ä¿¡åº¦ï¼Œè§‚æœ›",
            "",
            "ä¸è¦æœºæ¢°è¦æ±‚'å¿…é¡»3ä¸ªç»´åº¦'æˆ–'ADXå¿…é¡»>25'ï¼š",
            "  æœ‰æ—¶2ä¸ªå¼ºä¿¡å·æ¯”3ä¸ªå¼±ä¿¡å·æ›´å¥½",
            "  ADX 23ä¸”åœ¨ä¸Šå‡ï¼Œå¯èƒ½æ¯”ADX 26ä½†åœ¨ä¸‹é™æ›´å¥½",
            "  å¸‚åœºæ˜¯åŠ¨æ€çš„ï¼Œçµæ´»åˆ¤æ–­",
            "",
            "ã€ä»“ä½å¤§å°ã€‘",
            "",
            "æ ¹æ®ç½®ä¿¡åº¦å†³å®šæŠ•å…¥é‡‘é¢ï¼š",
            "",
            "é«˜ç½®ä¿¡åº¦ï¼šå¤šç»´åº¦ç¡®è®¤ï¼Œè¶‹åŠ¿å¼ºåŠ²",
            "  å¯è€ƒè™‘æŠ•å…¥300-400 USDTï¼ˆå‡è®¾ä½™é¢1000 USDTï¼‰",
            "  ä¾‹å¦‚ï¼š4h+1hè¶‹åŠ¿ä¸€è‡´ï¼ŒADX>25ä¸”ä¸Šå‡ï¼Œçªç ´ç¡®è®¤",
            "",
            "ä¸­ç­‰ç½®ä¿¡åº¦ï¼šéƒ¨åˆ†ç¡®è®¤ï¼Œè¶‹åŠ¿ä¸­ç­‰",
            "  å¯è€ƒè™‘æŠ•å…¥200-300 USDTï¼ˆå‡è®¾ä½™é¢1000 USDTï¼‰",
            "  ä¾‹å¦‚ï¼š4hè¶‹åŠ¿æ˜ç¡®ï¼Œ1hæœ‰å…¥åœºä¿¡å·ï¼ŒADX 20-25",
            "",
            "ä½ç½®ä¿¡åº¦ï¼šä¿¡å·è¾ƒå¼±ï¼Œä½†æœ‰æœºä¼š",
            "  å¯è€ƒè™‘æŠ•å…¥100-200 USDTï¼ˆå‡è®¾ä½™é¢1000 USDTï¼‰",
            "  ä¾‹å¦‚ï¼šè¶‹åŠ¿åˆç°ï¼Œä½†æœªå®Œå…¨ç¡®è®¤",
            "",
            "é€šè¿‡stake_amountå‚æ•°æŒ‡å®šå…·ä½“USDTé‡‘é¢ï¼š",
            "  æŸ¥çœ‹ã€è´¦æˆ·ä¿¡æ¯ã€‘ä¸­çš„å¯ç”¨ä½™é¢",
            "  æ ¹æ®ä½ çš„ç½®ä¿¡åº¦å’Œä½™é¢è®¡ç®—",
            "  å¦‚æœä¸è®¾ç½®ï¼Œç³»ç»Ÿä½¿ç”¨é»˜è®¤ä»“ä½",
            "",
            "ã€è¦é¿å…çš„æ€ç»´é™·é˜±ã€‘",
            "",
            "ä¸è¦åªçœ‹å•ä¸€æŒ‡æ ‡ï¼š",
            "  åªçœ‹MACDè½¬æ­£ï¼Œå¿½ç•¥EMAæ­»å‰ = ç‰‡é¢",
            "  åªçœ‹å¤šç©ºæ¯”æç«¯ï¼Œå¿½ç•¥ä»·æ ¼èµ°åŠ¿ = è¯¯å¯¼",
            "  ç»¼åˆå¤šä¸ªç»´åº¦ï¼Œä½†ä¸æœºæ¢°è¦æ±‚æ•°é‡",
            "",
            "ä¸è¦è¿‡åº¦ä¾èµ–è§„åˆ™ï¼š",
            "  ADX 23 vs 25ï¼Œçœ‹è¶‹åŠ¿æ˜¯å¦åœ¨å¢å¼º",
            "  å›è°ƒæœªåˆ°EMA20ï¼Œä½†å…¶ä»–ä¿¡å·å¼ºçƒˆï¼Œä¹Ÿå¯è€ƒè™‘",
            "  å¸‚åœºæ˜¯åŠ¨æ€çš„ï¼Œçµæ´»åº”å¯¹",
            "",
            "ä¸è¦é€†åŠ¿è€Œä¸ºï¼ˆé™¤éç¡®è®¤åè½¬ï¼‰ï¼š",
            "  4å°æ—¶ç©ºå¤´æ—¶åšå¤šé£é™©æé«˜",
            "  ä½†è¦è¯†åˆ«è¶‹åŠ¿è½¬æŠ˜ç‚¹",
            "  ç»“æ„æ”¹å˜ + å¤šç»´åº¦ç¡®è®¤ = å¯èƒ½çš„åè½¬",
            "",
            "ã€æ ¸å¿ƒç†å¿µã€‘",
            "",
            "å¼€ä»“ç†ç”±æ˜¯æŒä»“ç®¡ç†çš„é”šç‚¹ï¼Œå¿…é¡»æ¸…æ™°ã€‚",
            "å¤šç»´åº¦éªŒè¯ï¼Œä½†çµæ´»åˆ¤æ–­ï¼Œä¸æœºæ¢°æ‰§è¡Œã€‚",
            "è¶‹åŠ¿æ˜¯æœ‹å‹ï¼Œä½†è¦è¯†åˆ«è¶‹åŠ¿çš„ä¸åŒé˜¶æ®µã€‚",
            "æœ‰ç–‘è™‘æ—¶å®å¯ç­‰å¾…ï¼Œæœºä¼šæ°¸è¿œéƒ½æœ‰ã€‚",
            "",
            "åˆ†æå®Œæ¯•åï¼Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°åšå†³ç­–ã€‚",
        ]

        return "\n".join(parts)

    def _build_position_system_prompt_old(self) -> str:
        """
        æ—§ç‰ˆæŒä»“æç¤ºè¯ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå‚è€ƒï¼‰

        Returns:
            æŒä»“ç®¡ç†ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        parts = [
            "ä½ æ˜¯ä¸“ä¸šçš„åŠ å¯†è´§å¸è¶‹åŠ¿äº¤æ˜“å‘˜ã€‚ä»»åŠ¡ï¼šç®¡ç†ç°æœ‰æŒä»“ã€‚",
            "",
            "ã€æ ¸å¿ƒåŸåˆ™ã€‘",
            "",
            "ç›®æ ‡ï¼šåƒå®Œæ•´è¶‹åŠ¿ï¼Œä¸è¦åŠè·¯ä¸‹è½¦ã€‚",
            "å¼€ä»“ç†ç”±æ˜¯ä½ çš„é”šç‚¹ï¼Œåªæœ‰é”šç‚¹è¢«ç ´åæ—¶æ‰è€ƒè™‘å¹³ä»“ã€‚",
            "",
            "ã€å¼€ä»“é”šç‚¹åˆ†æã€‘",
            "",
            "é¦–å…ˆå›é¡¾å¼€ä»“ç†ç”±ï¼š",
            "  æŸ¥çœ‹'å¼€ä»“ç†ç”±'ï¼Œæå–æ ¸å¿ƒä¾æ®ï¼š",
            "    æ˜¯åŸºäºEMAçªç ´ï¼Ÿ",
            "    æ˜¯åŸºäºè¶‹åŠ¿å½¢æˆï¼Ÿ",
            "    æ˜¯åŸºäºå…³é”®æ”¯æ’‘ä½ï¼Ÿ",
            "    æ˜¯åŸºäºå¤šæ—¶é—´æ¡†æ¶å¯¹é½ï¼Ÿ",
            "",
            "åˆ¤æ–­é”šç‚¹æ˜¯å¦è¿˜æœ‰æ•ˆï¼š",
            "  å¦‚æœå¼€ä»“ç†ç”±æ˜¯'4å°æ—¶å¤šå¤´è¶‹åŠ¿ï¼Œä»·æ ¼å›è¸©EMA20åå¼¹'ï¼š",
            "    æ£€æŸ¥ï¼š4å°æ—¶EMAæ’åˆ—æ˜¯å¦è¿˜æ˜¯å¤šå¤´ï¼ˆEMA20>50>100ï¼‰ï¼Ÿ",
            "    æ£€æŸ¥ï¼šä»·æ ¼æ˜¯å¦è¿˜åœ¨EMA20ä¸Šæ–¹ï¼Ÿ",
            "    å¦‚æœéƒ½æ»¡è¶³ = é”šç‚¹æœ‰æ•ˆ = ç»§ç»­æŒæœ‰",
            "    å¦‚æœä»»ä¸€ä¸æ»¡è¶³ = é”šç‚¹ç ´å = è€ƒè™‘å¹³ä»“",
            "",
            "  å¦‚æœå¼€ä»“ç†ç”±æ˜¯'çªç ´å…³é”®é˜»åŠ›ä½XXX'ï¼š",
            "    æ£€æŸ¥ï¼šä»·æ ¼æ˜¯å¦è¿˜åœ¨é˜»åŠ›ä½ä¸Šæ–¹ï¼Ÿ",
            "    æ£€æŸ¥ï¼šæ˜¯å¦ç«™ç¨³è‡³å°‘3æ ¹Kçº¿ï¼Ÿ",
            "    å¦‚æœè·Œç ´å¹¶ç«™ç¨³ä¸‹æ–¹ = å‡çªç ´ = ç«‹å³å¹³ä»“",
            "",
            "ã€åŒºåˆ†å›è°ƒä¸åè½¬ã€‘",
            "",
            "è¶‹åŠ¿çš„ç»“æ„ç‰¹å¾ï¼š",
            "  å¤šå¤´è¶‹åŠ¿ = Higher Highs + Higher Lowsï¼ˆæ¯æ¬¡å›è°ƒä½ç‚¹éƒ½æ¯”ä¸Šæ¬¡é«˜ï¼‰",
            "  ç©ºå¤´è¶‹åŠ¿ = Lower Highs + Lower Lowsï¼ˆæ¯æ¬¡åå¼¹é«˜ç‚¹éƒ½æ¯”ä¸Šæ¬¡ä½ï¼‰",
            "",
            "åˆ¤æ–­æ–¹æ³•ï¼š",
            "  çœ‹Kçº¿å†å²æ•°æ®ï¼Œè¯†åˆ«æœ€è¿‘çš„é«˜ç‚¹å’Œä½ç‚¹ã€‚",
            "  å¤šå¤´è¶‹åŠ¿ä¸­ï¼Œå¦‚æœå‡ºç°Lower Highï¼ˆæ–°é«˜ç‚¹ä½äºå‰ä¸€é«˜ç‚¹ï¼‰= è¶‹åŠ¿å¯èƒ½åè½¬",
            "  ç©ºå¤´è¶‹åŠ¿ä¸­ï¼Œå¦‚æœå‡ºç°Higher Lowï¼ˆæ–°ä½ç‚¹é«˜äºå‰ä¸€ä½ç‚¹ï¼‰= è¶‹åŠ¿å¯èƒ½åè½¬",
            "",
            "å›è°ƒçš„ç‰¹å¾ï¼š",
            "  ä»·æ ¼çŸ­æš‚å›æ’¤ï¼Œä½†ä¸ç ´åè¶‹åŠ¿ç»“æ„",
            "  æ”¯æ’‘ä½ï¼ˆå¤šå¤´ï¼‰æˆ–é˜»åŠ›ä½ï¼ˆç©ºå¤´ï¼‰å®ˆä½",
            "  EMAæ’åˆ—ä¸å˜",
            "  ADXè™½ç„¶ä¸‹é™ä½†ä»>20",
            "  å›è°ƒåä»·æ ¼é‡æ–°æœè¶‹åŠ¿æ–¹å‘è¿åŠ¨",
            "",
            "åè½¬çš„ç‰¹å¾ï¼š",
            "  ä»·æ ¼ç»“æ„è¢«ç ´åï¼ˆå‡ºç°Lower Highæˆ–Higher Lowï¼‰",
            "  å…³é”®æ”¯æ’‘/é˜»åŠ›ä½è¢«çªç ´å¹¶ç«™ç¨³",
            "  EMAå¼€å§‹æ‹å¤´ï¼ˆEMA20ç©¿è¶ŠEMA50ï¼‰",
            "  MACDåœ¨é›¶è½´é™„è¿‘é‡‘å‰/æ­»å‰",
            "  ADXæŒç»­ä¸‹é™è‡³<20",
            "",
            "ã€æŒ‡æ ‡è§£è¯»ã€‘",
            "",
            "å•æ ¹Kçº¿çš„æŒ‡æ ‡æ³¢åŠ¨ä¸é‡è¦ï¼Œçœ‹è¶‹åŠ¿ã€‚",
            "",
            "MACDï¼š",
            "  å¤šå¤´è¶‹åŠ¿ä¸­ï¼ŒMACDæŸ±çŠ¶å›¾è½¬è´Ÿæ˜¯æ­£å¸¸å›è°ƒï¼Œç­‰å®ƒé‡æ–°è½¬æ­£",
            "  åªæœ‰MACDåœ¨é›¶è½´é™„è¿‘æ­»å‰ä¸”ä»·æ ¼è·Œç ´EMA20 = è­¦æƒ•åè½¬",
            "  ç©ºå¤´è¶‹åŠ¿ä¸­ï¼ŒMACDæŸ±çŠ¶å›¾è½¬æ­£æ˜¯æ­£å¸¸åå¼¹ï¼Œç­‰å®ƒé‡æ–°è½¬è´Ÿ",
            "",
            "RSIï¼š",
            "  å¼ºè¶‹åŠ¿ä¸­RSIå¯ä»¥é•¿æœŸè¶…ä¹°ï¼ˆ>70ï¼‰æˆ–è¶…å–ï¼ˆ<30ï¼‰",
            "  å¤šå¤´è¶‹åŠ¿ä¸­RSIä»80é™åˆ°60 = æ­£å¸¸å›è°ƒï¼Œä¸æ˜¯å–å‡ºä¿¡å·",
            "  ç©ºå¤´è¶‹åŠ¿ä¸­RSIä»20å‡åˆ°40 = æ­£å¸¸åå¼¹ï¼Œä¸æ˜¯ä¹°å…¥ä¿¡å·",
            "",
            "ADXï¼š",
            "  ADX>25ï¼šè¶‹åŠ¿å¼ºåŠ²ï¼Œç»§ç»­æŒæœ‰",
            "  ADX 20-25ï¼šè¶‹åŠ¿å‡å¼±ä½†ä»åœ¨ï¼Œè§‚å¯Ÿ",
            "  ADX<20ï¼šè¶‹åŠ¿æ¶ˆå¤±ï¼Œè€ƒè™‘å¹³ä»“",
            "",
            "EMAï¼š",
            "  ä»·æ ¼å›è¸©EMA20ä½†ä¸ç ´ = å¥åº·å›è°ƒ",
            "  ä»·æ ¼è·Œç ´EMA20å¹¶ç«™ç¨³2æ ¹Kçº¿ä»¥ä¸Š = è¶‹åŠ¿å‡å¼±",
            "  EMA20è·Œç ´EMA50ï¼ˆæ­»å‰ï¼‰= è¶‹åŠ¿åè½¬ä¿¡å·",
            "",
            "ã€åŠ ä»“ç­–ç•¥ã€‘",
            "",
            "åŠ ä»“ï¼ˆadjust_positionï¼‰- é™ä½å¹³å‡æˆæœ¬æˆ–æ‰©å¤§ç›ˆåˆ©ã€‚",
            "",
            "å…è®¸åŠ ä»“çš„æƒ…å†µï¼š",
            "  è¶‹åŠ¿å¼ºåŠ²ï¼ˆADX>30ï¼‰ä¸”å¼€ä»“é”šç‚¹æ›´åŠ å¼ºåŒ–",
            "  å½“å‰æŒä»“ç›ˆåˆ©>3%ï¼Œä¸”ä»·æ ¼å›è°ƒåˆ°å¼ºæ”¯æ’‘ä½",
            "  ä¾‹å¦‚ï¼šåšå¤šæ—¶ï¼Œ4å°æ—¶EMAé‡‘å‰åï¼Œä»·æ ¼å›è¸©1å°æ—¶EMA20åå¼¹ = å¯åŠ ä»“",
            "",
            "åŠ ä»“å¹…åº¦ï¼š",
            "  æ ¹æ®è¶‹åŠ¿å¼ºåº¦å’Œå½“å‰ç›ˆåˆ©å†³å®šï¼š",
            "    è¶‹åŠ¿å¼ºï¼ˆADX>30ï¼‰+ å½“å‰ç›ˆåˆ©>5% = å¯åŠ ä»“50-100%ï¼ˆç›¸å¯¹å½“å‰ä»“ä½ï¼‰",
            "    è¶‹åŠ¿ä¸­ç­‰ï¼ˆADX 25-30ï¼‰+ å½“å‰ç›ˆåˆ©3-5% = å¯åŠ ä»“30-50%ï¼ˆç›¸å¯¹å½“å‰ä»“ä½ï¼‰",
            "  ä¾‹å¦‚ï¼šå½“å‰ä»“ä½100 USDTï¼ŒåŠ ä»“50%ï¼Œåˆ™å¢åŠ 50 USDT",
            "",
            "ä¸å…è®¸åŠ ä»“çš„æƒ…å†µï¼š",
            "  å½“å‰æŒä»“äºæŸ = ç»å¯¹ä¸åŠ ä»“ï¼ˆä¸è¦æ‘Šå¹³äºæŸï¼‰",
            "  è¶‹åŠ¿å‡å¼±ï¼ˆADX<25ï¼‰",
            "  æŒä»“æ—¶é—´<1å°æ—¶ï¼ˆè¶‹åŠ¿æœªç¡®è®¤ï¼‰",
            "",
            "åŠ ä»“æ–¹å¼ï¼š",
            "  è°ƒç”¨adjust_positionå‡½æ•°ï¼Œå‚æ•°ï¼š",
            "    pair: äº¤æ˜“å¯¹",
            "    adjustment_pct: åŠ ä»“ç™¾åˆ†æ¯”ï¼ˆ50è¡¨ç¤ºåŠ ä»“50%ï¼‰",
            "    limit_price: æœŸæœ›çš„åŠ ä»“ä»·æ ¼",
            "    confidence_score: ç½®ä¿¡åº¦ï¼ˆ1-100ï¼‰",
            "    key_support: å…³é”®æ”¯æ’‘ä½",
            "    key_resistance: å…³é”®é˜»åŠ›ä½",
            "    reason: åŠ ä»“ç†ç”±",
            "",
            "ã€æŒä»“æ—¶é—´ä¸ç›ˆåˆ©ã€‘",
            "",
            "è¶‹åŠ¿éœ€è¦æ—¶é—´å‘å±•ã€‚",
            "  æŒä»“<1å°æ—¶ = è¶‹åŠ¿æœªå±•å¼€ï¼Œä¸è¦å› ä¸ºå°å¹…æ³¢åŠ¨å°±å¹³ä»“",
            "  æŒä»“1-3å°æ—¶ = è¶‹åŠ¿å‘å±•ä¸­ï¼Œåªè¦é”šç‚¹æœ‰æ•ˆå°±æŒæœ‰",
            "  æŒä»“>3å°æ—¶ = å¦‚æœç›ˆåˆ©è¾¾æ ‡ï¼ˆ>5%ï¼‰ä¸”è¶‹åŠ¿å‡å¼±ï¼Œå¯ä»¥è·åˆ©äº†ç»“",
            "",
            "ç›ˆåˆ©å›æ’¤å¤„ç†ï¼š",
            "  æŸ¥çœ‹'ç›ˆåˆ©å›æ’¤'æ•°æ®ï¼š",
            "    å¦‚æœä»å³°å€¼å›æ’¤>30%ï¼ˆå¦‚ä»+8%å›æ’¤åˆ°+5.5% = å›æ’¤31%ï¼‰",
            "    ä¸”ADX<25ï¼Œè€ƒè™‘è·åˆ©äº†ç»“",
            "",
            "ã€å†³ç­–æµç¨‹ã€‘",
            "",
            "æŒ‰é¡ºåºæ‰§è¡Œï¼š",
            "",
            "1. å›é¡¾å¼€ä»“é”šç‚¹",
            "   å¼€ä»“ç†ç”±ä¸­çš„æ ¸å¿ƒä¾æ®è¿˜æˆç«‹å—ï¼Ÿ",
            "   å¦‚æœä¸æˆç«‹ = è°ƒç”¨signal_exit",
            "",
            "2. åˆ¤æ–­è¶‹åŠ¿çŠ¶æ€",
            "   çœ‹ä»·æ ¼ç»“æ„ï¼ˆHigher Highs/Lower Lowsï¼‰",
            "   çœ‹EMAæ’åˆ—ã€MACDæ–¹å‘ã€ADXå¼ºåº¦",
            "   å¦‚æœè¶‹åŠ¿åè½¬ = è°ƒç”¨signal_exit",
            "   å¦‚æœæ˜¯å›è°ƒ = ç»§ç»­æŒæœ‰",
            "",
            "3. è¯„ä¼°åŠ ä»“æœºä¼š",
            "   å¦‚æœç›ˆåˆ©>3% ä¸” è¶‹åŠ¿å¼ºåŒ– ä¸” ä»·æ ¼å›è°ƒåˆ°æ”¯æ’‘",
            "   = è€ƒè™‘è°ƒç”¨adjust_positionåŠ ä»“",
            "",
            "4. è¯„ä¼°è·åˆ©äº†ç»“",
            "   å¦‚æœç›ˆåˆ©>5% ä¸”ï¼ˆè¶‹åŠ¿å‡å¼± æˆ– æŒä»“>3å°æ—¶ æˆ– è¾¾åˆ°ç›®æ ‡ä½ï¼‰",
            "   = è°ƒç”¨signal_exit",
            "",
            "5. å¦‚æœä»¥ä¸Šéƒ½ä¸æ»¡è¶³",
            "   = è°ƒç”¨signal_holdï¼Œè¯´æ˜æŒæœ‰ç†ç”±ï¼ˆé”šç‚¹æœ‰æ•ˆã€è¶‹åŠ¿æœªå˜ï¼‰",
            "",
            "ã€å¸¸è§é”™è¯¯ã€‘",
            "",
            "å¿…é¡»é¿å…ï¼š",
            "  è¢«å•æ ¹Kçº¿çš„æŒ‡æ ‡å˜åŒ–å“è·‘ï¼ˆMACDè½¬è´Ÿã€RSIå›è½ï¼‰",
            "  æ­£å¸¸å›è°ƒæ—¶è¿‡æ—©å¹³ä»“ï¼ˆä»·æ ¼å›è¸©EMA20å°±è·‘ï¼‰",
            "  æŒä»“æ—¶é—´å¤ªçŸ­ï¼ˆ<30åˆ†é’Ÿå°±æƒ³è·åˆ©ï¼‰",
            "  äºæŸæ—¶åŠ ä»“ï¼ˆè¯•å›¾æ‘Šå¹³æˆæœ¬ï¼‰",
            "  ç›ˆåˆ©æ—¶è¿‡åº¦è´ªå©ªï¼ˆè¶‹åŠ¿å·²åè½¬è¿˜ä¸èµ°ï¼‰",
            "  å¿˜è®°å¼€ä»“é”šç‚¹ï¼Œåªçœ‹å½“å‰æŒ‡æ ‡",
            "",
            "ã€è®°ä½ã€‘",
            "",
            "ä½ æ˜¯è¶‹åŠ¿äº¤æ˜“å‘˜ï¼Œä¸æ˜¯çŸ­çº¿å®¢ã€‚",
            "ç»™è¶‹åŠ¿æ—¶é—´ï¼Œä¸è¦è¢«çŸ­æœŸæ³¢åŠ¨å¹²æ‰°ã€‚",
            "åªæœ‰é”šç‚¹ç ´åæˆ–è¶‹åŠ¿çœŸæ­£åè½¬æ—¶æ‰ç¦»åœºã€‚",
            "",
            "åˆ†æå®Œæ¯•åï¼Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°åšå†³ç­–ã€‚",
        ]

        return "\n".join(parts)

    def build_decision_request(
        self,
        action_type: str,
        market_context: str,
        position_context: str
    ) -> str:
        """
        æ„å»ºå†³ç­–è¯·æ±‚

        Args:
            action_type: å†³ç­–ç±»å‹ (entry/exit)
            market_context: å¸‚åœºä¸Šä¸‹æ–‡ï¼ˆå·²åŒ…å«å†å²ç»éªŒï¼‰
            position_context: æŒä»“ä¸Šä¸‹æ–‡

        Returns:
            å®Œæ•´çš„å†³ç­–è¯·æ±‚å­—ç¬¦ä¸²
        """
        action_desc = {
            'entry': 'æ˜¯å¦åº”è¯¥å¼€ä»“(åšå¤šæˆ–åšç©º)',
            'exit': 'æ˜¯å¦åº”è¯¥å¹³ä»“'
        }

        request_parts = [
            f"è¯·åˆ†æå½“å‰æƒ…å†µï¼Œå†³ç­–: {action_desc.get(action_type, action_type)}",
            "",
            "=" * 50,
            market_context,
            "",
            "=" * 50,
            position_context,
            "",
            "å†³ç­–åè°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Œç«‹å³åœæ­¢"
        ]

        return "\n".join(request_parts)

    def optimize_token_usage(self, text: str, max_tokens: int) -> str:
        """
        ä¼˜åŒ–æ–‡æœ¬ä»¥é€‚åº”tokené™åˆ¶

        Args:
            text: åŸå§‹æ–‡æœ¬
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            ä¼˜åŒ–åçš„æ–‡æœ¬
        """
        # ç®€å•ä¼°ç®—: 1 token â‰ˆ 4 å­—ç¬¦ (è‹±æ–‡) æˆ– 1.5 å­—ç¬¦ (ä¸­æ–‡)
        estimated_tokens = len(text) / 2.5

        if estimated_tokens <= max_tokens:
            return text

        # éœ€è¦æˆªæ–­
        target_chars = int(max_tokens * 2.5)
        truncated = text[:target_chars]

        # åœ¨æœ€åä¸€ä¸ªæ¢è¡Œç¬¦å¤„æˆªæ–­ï¼Œé¿å…æˆªæ–­å¥å­
        last_newline = truncated.rfind('\n')
        if last_newline > target_chars * 0.8:  # å¦‚æœä¸ä¼šæŸå¤±å¤ªå¤š
            truncated = truncated[:last_newline]

        return truncated + "\n... (å†…å®¹å·²æˆªæ–­)"

    def _analyze_trend(self, prices: List[float]) -> Dict[str, Any]:
        """
        åˆ†æä»·æ ¼è¶‹åŠ¿ï¼ˆæ”¹è¿›ç‰ˆï¼‰

        Returns:
            DictåŒ…å«: direction (æ–¹å‘), strength (å¼ºåº¦), change_pct (å˜åŒ–ç™¾åˆ†æ¯”)
        """
        if len(prices) < 2:
            return {
                "direction": "æœªçŸ¥",
                "strength": "æ— ",
                "change_pct": 0
            }

        # è®¡ç®—ä»·æ ¼å˜åŒ–
        total_change = (prices[-1] - prices[0]) / prices[0] * 100

        # ç¡®å®šæ–¹å‘ï¼ˆé™ä½é—¨æ§›ï¼šä»Â±1%é™åˆ°Â±0.5%ï¼‰
        if total_change > 0.5:
            direction = "ä¸Šæ¶¨"
        elif total_change < -0.5:
            direction = "ä¸‹è·Œ"
        else:
            direction = "æ¨ªç›˜"

        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦ï¼ˆåŸºäºå¹…åº¦ï¼‰
        abs_change = abs(total_change)
        if abs_change > 3:
            strength = "å¼ºåŠ¿"
        elif abs_change > 1:
            strength = "ä¸­ç­‰"
        elif abs_change > 0.5:
            strength = "å¼±åŠ¿"
        else:
            strength = "æå¼±"

        # è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§ï¼ˆä»·æ ¼æ˜¯å¦æœåŒä¸€æ–¹å‘ç§»åŠ¨ï¼‰
        changes = [prices[i] - prices[i-1] for i in range(1, len(prices))]
        if direction != "æ¨ªç›˜":
            same_direction = sum(1 for c in changes if (c > 0 and direction == "ä¸Šæ¶¨") or (c < 0 and direction == "ä¸‹è·Œ"))
            consistency = same_direction / len(changes) * 100
        else:
            consistency = 0

        return {
            "direction": direction,
            "strength": strength,
            "change_pct": total_change,
            "consistency": consistency
        }

    def _format_duration(self, start_time: datetime) -> str:
        """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
        if not start_time:
            return "æœªçŸ¥"

        from datetime import timezone, datetime as dt
        # freqtradeä½¿ç”¨naive UTCæ—¶é—´ï¼Œæ ¹æ®æ˜¯å¦æœ‰tzinfoé€‰æ‹©å¯¹åº”çš„now
        now = dt.utcnow() if start_time.tzinfo is None else dt.now(timezone.utc)
        duration = now - start_time
        hours = duration.total_seconds() / 3600

        if hours < 1:
            return f"{int(hours * 60)}åˆ†é’Ÿ"
        elif hours < 24:
            return f"{hours:.1f}å°æ—¶"
        else:
            return f"{hours / 24:.1f}å¤©"

    def _analyze_market_structure(self, dataframe: pd.DataFrame, lookback: int = 100) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºç»“æ„ï¼ˆæ”¯æ’‘/é˜»åŠ›/è¶‹åŠ¿ï¼‰- lookback=100æ ¹Kçº¿(25å°æ—¶)èƒ½æ‰¾åˆ°æ›´é•¿æœŸçš„æ”¯æ’‘é˜»åŠ›"""
        if len(dataframe) < lookback:
            return {"structure": "æ•°æ®ä¸è¶³"}

        recent = dataframe.tail(lookback)

        # è®¡ç®—æ‘†åŠ¨é«˜ä½ç‚¹
        highs = recent['high']
        lows = recent['low']
        closes = recent['close']

        swing_high = highs.max()
        swing_low = lows.min()
        current_price = closes.iloc[-1]

        # åˆ¤æ–­å¸‚åœºç»“æ„
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆ›æ–°é«˜/æ–°ä½
        prev_highs = dataframe['high'].tail(lookback*2).head(lookback)
        prev_lows = dataframe['low'].tail(lookback*2).head(lookback)

        is_higher_high = swing_high > prev_highs.max() if len(prev_highs) > 0 else False
        is_lower_low = swing_low < prev_lows.min() if len(prev_lows) > 0 else False

        # ç¡®å®šç»“æ„
        if is_higher_high and not is_lower_low:
            structure = "ä¸Šå‡ç»“æ„ï¼ˆHigher Highsï¼‰"
        elif is_lower_low and not is_higher_high:
            structure = "ä¸‹é™ç»“æ„ï¼ˆLower Lowsï¼‰"
        elif is_higher_high and is_lower_low:
            structure = "æ‰©å¼ ç»“æ„ï¼ˆéœ‡è¡åŠ å‰§ï¼‰"
        else:
            structure = "ç›˜æ•´ç»“æ„ï¼ˆéœ‡è¡ï¼‰"

        # è·ç¦»å…³é”®ä½çš„ç™¾åˆ†æ¯”
        distance_to_high = ((swing_high - current_price) / current_price) * 100
        distance_to_low = ((current_price - swing_low) / current_price) * 100

        return {
            "structure": structure,
            "swing_high": swing_high,
            "swing_low": swing_low,
            "distance_to_high_pct": distance_to_high,
            "distance_to_low_pct": distance_to_low,
            "range_pct": ((swing_high - swing_low) / swing_low) * 100
        }

    def _analyze_indicator_trends(self, dataframe: pd.DataFrame) -> Dict[str, str]:
        """åˆ†ææŒ‡æ ‡å˜åŒ–è¶‹åŠ¿"""
        if len(dataframe) < 5:
            return {}

        latest = dataframe.iloc[-1]
        prev = dataframe.iloc[-2]
        prev_5 = dataframe.iloc[-5]

        trends = {}

        # EMAäº¤å‰çŠ¶æ€
        if 'ema_20' in latest and 'ema_50' in latest:
            ema20_now = latest['ema_20']
            ema50_now = latest['ema_50']
            ema20_prev = prev['ema_20']
            ema50_prev = prev['ema_50']

            if ema20_now > ema50_now and ema20_prev <= ema50_prev:
                trends['ema_cross'] = "åˆšåˆšé‡‘å‰"
            elif ema20_now < ema50_now and ema20_prev >= ema50_prev:
                trends['ema_cross'] = "åˆšåˆšæ­»å‰"
            elif ema20_now > ema50_now:
                trends['ema_cross'] = "é‡‘å‰æŒç»­ä¸­"
            else:
                trends['ema_cross'] = "æ­»å‰æŒç»­ä¸­"

        # MACDæŸ±çŠ¶å›¾è¶‹åŠ¿
        if 'macd_hist' in latest:
            macd_hist_now = latest['macd_hist']
            macd_hist_prev = prev['macd_hist']

            if macd_hist_now > macd_hist_prev:
                trends['macd_histogram'] = "å¢å¼ºï¼ˆå¤šå¤´ï¼‰" if macd_hist_now > 0 else "å‡å¼±ï¼ˆç©ºå¤´å¼±åŒ–ï¼‰"
            else:
                trends['macd_histogram'] = "å‡å¼±ï¼ˆå¤šå¤´å¼±åŒ–ï¼‰" if macd_hist_now > 0 else "å¢å¼ºï¼ˆç©ºå¤´ï¼‰"

        # RSIè¶‹åŠ¿
        if 'rsi' in latest:
            rsi_now = latest['rsi']
            rsi_5ago = prev_5['rsi']

            if rsi_now > rsi_5ago + 5:
                trends['rsi_trend'] = "ä¸Šå‡ï¼ˆåŠ¨èƒ½å¢å¼ºï¼‰"
            elif rsi_now < rsi_5ago - 5:
                trends['rsi_trend'] = "ä¸‹é™ï¼ˆåŠ¨èƒ½å‡å¼±ï¼‰"
            else:
                trends['rsi_trend'] = "å¹³ç¨³"

        # ADXè¶‹åŠ¿å¼ºåº¦
        if 'adx' in latest:
            adx_now = latest['adx']
            adx_prev = prev['adx']

            if adx_now > adx_prev:
                trends['adx_direction'] = f"ä¸Šå‡ï¼ˆè¶‹åŠ¿å¢å¼ºï¼Œå½“å‰{adx_now:.1f}ï¼‰"
            else:
                trends['adx_direction'] = f"ä¸‹é™ï¼ˆè¶‹åŠ¿å‡å¼±ï¼Œå½“å‰{adx_now:.1f}ï¼‰"

        return trends

    def _analyze_timeframe_alignment(self, dataframe: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æå¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿å¯¹é½ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ”¯æŒæ¨ªç›˜ï¼‰
        """
        if len(dataframe) < 2:
            return {"alignment": "æ•°æ®ä¸è¶³", "trends": {}}

        latest = dataframe.iloc[-1]
        trends = {}

        # æ¨ªç›˜åˆ¤æ–­é˜ˆå€¼ï¼šEMAå·®è·å°äº0.3%è§†ä¸ºæ¨ªç›˜
        consolidation_threshold = 0.003

        # 15åˆ†é’Ÿè¶‹åŠ¿
        if 'ema_20' in latest and 'ema_50' in latest:
            ema_diff = (latest['ema_20'] - latest['ema_50']) / latest['ema_50']
            if abs(ema_diff) < consolidation_threshold:
                trends['15m'] = "æ¨ªç›˜"
            elif ema_diff > 0:
                trends['15m'] = "ä¸Šæ¶¨"
            else:
                trends['15m'] = "ä¸‹è·Œ"

        # 1å°æ—¶è¶‹åŠ¿
        if 'ema_20_1h' in latest and 'ema_50_1h' in latest:
            ema_diff = (latest['ema_20_1h'] - latest['ema_50_1h']) / latest['ema_50_1h']
            if abs(ema_diff) < consolidation_threshold:
                trends['1h'] = "æ¨ªç›˜"
            elif ema_diff > 0:
                trends['1h'] = "ä¸Šæ¶¨"
            else:
                trends['1h'] = "ä¸‹è·Œ"

        # 4å°æ—¶è¶‹åŠ¿
        if 'ema_20_4h' in latest and 'ema_50_4h' in latest:
            ema_diff = (latest['ema_20_4h'] - latest['ema_50_4h']) / latest['ema_50_4h']
            if abs(ema_diff) < consolidation_threshold:
                trends['4h'] = "æ¨ªç›˜"
            elif ema_diff > 0:
                trends['4h'] = "ä¸Šæ¶¨"
            else:
                trends['4h'] = "ä¸‹è·Œ"

        # 1å¤©è¶‹åŠ¿
        if 'ema_20_1d' in latest and 'ema_50_1d' in latest:
            ema_diff = (latest['ema_20_1d'] - latest['ema_50_1d']) / latest['ema_50_1d']
            if abs(ema_diff) < consolidation_threshold:
                trends['1d'] = "æ¨ªç›˜"
            elif ema_diff > 0:
                trends['1d'] = "ä¸Šæ¶¨"
            else:
                trends['1d'] = "ä¸‹è·Œ"

        # åˆ¤æ–­å¯¹é½æƒ…å†µ
        if not trends:
            return {"alignment": "æ— è¶‹åŠ¿æ•°æ®", "trends": {}}

        uptrend_count = sum(1 for t in trends.values() if t == "ä¸Šæ¶¨")
        downtrend_count = sum(1 for t in trends.values() if t == "ä¸‹è·Œ")
        consolidation_count = sum(1 for t in trends.values() if t == "æ¨ªç›˜")
        total_count = len(trends)

        if uptrend_count == total_count:
            alignment = "å®Œå…¨å¯¹é½ - å¼ºåŠ¿ä¸Šæ¶¨"
        elif downtrend_count == total_count:
            alignment = "å®Œå…¨å¯¹é½ - å¼ºåŠ¿ä¸‹è·Œ"
        elif consolidation_count == total_count:
            alignment = "å®Œå…¨å¯¹é½ - æ¨ªç›˜æ•´ç†"
        elif consolidation_count >= total_count / 2:
            alignment = f"æ¨ªç›˜ä¸ºä¸»ï¼ˆ{consolidation_count}/{total_count}ï¼‰- ç­‰å¾…çªç ´"
        elif uptrend_count > downtrend_count:
            alignment = f"å¤šæ•°ä¸Šæ¶¨ï¼ˆ{uptrend_count}/{total_count}ï¼‰"
        elif downtrend_count > uptrend_count:
            alignment = f"å¤šæ•°ä¸‹è·Œï¼ˆ{downtrend_count}/{total_count}ï¼‰"
        else:
            alignment = "è¶‹åŠ¿åˆ†æ­§ï¼ˆéœ‡è¡ï¼‰"

        return {
            "trends": trends,
            "alignment": alignment,
            "strength": uptrend_count / total_count if uptrend_count > downtrend_count else -downtrend_count / total_count
        }

    def _analyze_volume_trend(self, dataframe: pd.DataFrame, lookback: int = 100) -> Dict[str, Any]:
        """åˆ†ææˆäº¤é‡è¶‹åŠ¿ - lookback=100æ ¹Kçº¿(25å°æ—¶)èƒ½æ›´å¥½è¯†åˆ«æ”¾é‡/ç¼©é‡è¶‹åŠ¿"""
        if len(dataframe) < lookback:
            return {"trend": "æ•°æ®ä¸è¶³"}

        recent = dataframe.tail(lookback)
        volumes = recent['volume']

        # è®¡ç®—æˆäº¤é‡ç§»åŠ¨å¹³å‡
        volume_ma = volumes.mean()
        current_volume = volumes.iloc[-1]

        # æˆäº¤é‡è¶‹åŠ¿
        first_half_avg = volumes.head(lookback//2).mean()
        second_half_avg = volumes.tail(lookback//2).mean()

        if second_half_avg > first_half_avg * 1.2:
            trend = "æŒç»­æ”¾é‡"
        elif second_half_avg < first_half_avg * 0.8:
            trend = "æŒç»­ç¼©é‡"
        else:
            trend = "å¹³ç¨³"

        # å½“å‰æˆäº¤é‡ç›¸å¯¹äºå¹³å‡å€¼
        volume_ratio = current_volume / volume_ma

        if volume_ratio > 1.5:
            current_status = "å¼‚å¸¸æ”¾é‡"
        elif volume_ratio > 1.2:
            current_status = "æ˜æ˜¾æ”¾é‡"
        elif volume_ratio < 0.7:
            current_status = "æ˜æ˜¾ç¼©é‡"
        else:
            current_status = "æ­£å¸¸"

        return {
            "trend": trend,
            "current_status": current_status,
            "volume_ratio": volume_ratio,
            "current_vs_avg": f"{(volume_ratio - 1) * 100:+.1f}%"
        }

    def _get_indicator_history(
        self,
        dataframe: pd.DataFrame,
        lookback: int = 100,
        display_points: int = 20
    ) -> Dict[str, List]:
        """è·å–å…³é”®æŒ‡æ ‡çš„å†å²åºåˆ— - å±•ç¤ºå¯é…ç½®æ•°é‡çš„æœ€æ–°å€¼"""
        if len(dataframe) < lookback:
            lookback = len(dataframe)

        recent = dataframe.tail(lookback)
        history = {}

        # é€‰æ‹©å…³é”®æŒ‡æ ‡æä¾›å†å²åºåˆ—
        key_indicators = ['close', 'rsi', 'macd', 'macd_hist', 'ema_20', 'ema_50', 'adx', 'volume']

        for ind in key_indicators:
            if ind in recent.columns:
                values = recent[ind].tolist()
                # åªä¿ç•™æœ€è¿‘çš„å€¼ï¼Œæ ¼å¼åŒ–ä¸ºç®€æ´å½¢å¼
                trimmed = values[-display_points:]
                history[f"{ind}_recent"] = [
                    round(float(v), 2) if pd.notna(v) else None
                    for v in trimmed
                ]

        return history

    def _get_raw_kline_history(
        self,
        dataframe: pd.DataFrame,
        count: int,
        extra_fields: Optional[List[str]] = None,
        compact: bool = False,
        stride: int = 1,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """è¿”å›æŒ‡å®šæ•°é‡çš„åŸå§‹Kçº¿æ•°æ®æ–‡æœ¬/è¡¨æ ¼"""
        result = {"rows": [], "header": None}

        if count <= 0 or dataframe.empty:
            return result

        available_cols = set(dataframe.columns)
        base_fields = [col for col in ['open', 'high', 'low', 'close', 'volume'] if col in available_cols]
        if len(base_fields) < 4:
            return result

        extra = [
            field for field in self._ensure_list(extra_fields)
            if field in available_cols and field not in base_fields
        ]

        columns = ['time'] + base_fields + extra

        stride = max(1, stride)
        total_available = min(len(dataframe), count)
        if max_rows:
            max_rows = max(1, max_rows)
            if total_available > max_rows:
                stride = max(stride, math.ceil(total_available / max_rows))

        fetch_count = total_available * stride if stride > 1 else total_available
        subset = dataframe.tail(fetch_count)

        if subset.empty:
            return result

        if stride > 1 and len(subset) > count:
            subset = subset.iloc[::-1][::stride].iloc[::-1]

        subset = subset.tail(total_available)
        if subset.empty:
            return result

        if compact:
            result['header'] = ",".join(columns)
        result['stride'] = stride

        for _, row in subset.iterrows():
            time_str = self._format_timestamp(row.get('date'), row.name)

            if compact:
                values = [time_str]
                for col in columns[1:]:
                    values.append(self._format_number(row.get(col)))
                result['rows'].append(",".join(values))
            else:
                pieces = [
                    f"O:{self._format_number(row.get('open'))}",
                    f"H:{self._format_number(row.get('high'))}",
                    f"L:{self._format_number(row.get('low'))}",
                    f"C:{self._format_number(row.get('close'))}"
                ]

                if 'volume' in base_fields:
                    pieces.append(f"V:{self._format_number(row.get('volume'))}")

                if extra:
                    extras = [
                        f"{field}:{self._format_number(row.get(field))}"
                        for field in extra
                    ]
                    pieces.append(" ".join(extras))

                result['rows'].append(f"{time_str} | {' '.join(pieces)}")

        return result

    def get_multi_timeframe_history_config(self) -> Dict[str, Dict[str, Any]]:
        """å¯¹å¤–æä¾›å¤šæ—¶é—´æ¡†æ¶é…ç½®ï¼Œä¾›ç­–ç•¥å±‚å†³å®šéœ€è¦æ‹‰å–çš„æ•°æ®"""
        return self.multi_timeframe_history

    def _normalize_multi_timeframe_config(self, cfg: Any) -> Dict[str, Dict[str, Any]]:
        """å°†å¤šæ—¶é—´æ¡†æ¶é…ç½®æ ‡å‡†åŒ–ä¸º {tf: {candles:int, fields:list}}ï¼ˆå§”æ‰˜ç»™ DataFormatterï¼‰"""
        return self.formatter.normalize_multi_timeframe_config(cfg, self.config)

    def _normalize_multi_timeframe_config_old(self, cfg: Any) -> Dict[str, Dict[str, Any]]:
        """æ—§ç‰ˆé…ç½®æ ‡å‡†åŒ–ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå‚è€ƒï¼‰"""
        normalized: Dict[str, Dict[str, Any]] = {}
        if not cfg:
            return normalized

        default_fields = self._ensure_list(self.config.get("default_multi_timeframe_fields", []))

        if isinstance(cfg, dict):
            items = cfg.items()
        elif isinstance(cfg, list):
            items = []
            for entry in cfg:
                if isinstance(entry, dict) and entry.get('timeframe'):
                    tf = entry['timeframe']
                    data = entry.copy()
                    data.pop('timeframe', None)
                    items.append((tf, data))
        else:
            return normalized

        for tf, settings in items:
            if not tf:
                continue

            candles = 0
            fields = default_fields
            stride = 1
            compact = None
            max_rows = None

            if isinstance(settings, dict):
                candles = settings.get('candles') or settings.get('count') or settings.get('points') or settings.get('length') or 0
                fields = self._ensure_list(settings.get('fields') or settings.get('extra_fields') or default_fields)
                stride = settings.get('stride', 1)
                compact = settings.get('compact')
                max_rows = settings.get('max_rows')
            else:
                candles = settings
                fields = default_fields

            try:
                candles = int(candles)
            except (TypeError, ValueError):
                candles = 0

            try:
                stride = int(stride)
            except (TypeError, ValueError):
                stride = 1

            stride = max(1, stride)

            normalized[str(tf)] = {
                'candles': max(0, candles),
                'fields': fields,
                'stride': stride,
                'compact': compact,
                'max_rows': max_rows
            }

        return normalized

    def _format_timestamp(self, timestamp: Any, fallback_index: Any) -> str:
        if isinstance(timestamp, pd.Timestamp):
            ts = timestamp.to_pydatetime()
        elif isinstance(timestamp, datetime):
            ts = timestamp
        elif timestamp is None or (isinstance(timestamp, float) and pd.isna(timestamp)):
            if isinstance(fallback_index, pd.Timestamp):
                ts = fallback_index.to_pydatetime()
            elif isinstance(fallback_index, datetime):
                ts = fallback_index
            else:
                ts = None
        else:
            return str(timestamp)

        if isinstance(ts, datetime):
            return ts.strftime("%Y-%m-%d %H:%M")

        return str(timestamp if timestamp is not None else fallback_index)

    def _format_number(self, value: Any, decimals: int = 2) -> str:
        if value is None or pd.isna(value):
            return "-"

        try:
            number = float(value)
        except (TypeError, ValueError):
            return str(value)

        formatted = f"{number:.{decimals}f}"
        if '.' in formatted:
            formatted = formatted.rstrip('0').rstrip('.')
        return formatted or "0"

    @staticmethod
    def _ensure_list(value: Any) -> List[Any]:
        if value is None:
            return []
        if isinstance(value, list):
            return value
        return [value]
